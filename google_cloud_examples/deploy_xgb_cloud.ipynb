{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3",
   "metadata": {},
   "source": [
    "# Build, train, and deploy an XGBoost model on Cloud AI Platform\n",
    "\n",
    "**References**  \n",
    "* Coursera course: Machine Learning Engineering for\n",
    "Production (MLOps) Specialization. Link\n",
    "[here](https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops)\n",
    "* Notebook is accessed from\n",
    "[here](https://github.com/https-deeplearning-ai/machine-learning-engineering-for-production-public/blob/main/course4/week1-ungraded-labs/C4_W1_Optional_Lab_1_XGBoost_CAIP/C4_W1_Optional_Lab_1.md)\n",
    "\n",
    "**Note**\n",
    "\n",
    "If you cannot see the pictures, download the images folder and run locally!\n",
    "\n",
    "**Description** \n",
    "\n",
    "In this **optional** tutorial, will train an XGBoost model then see how\n",
    "you can use a managed service to deploy and serve this model in Google\n",
    "Cloud Platform (GCP). This is based on the Codelabs provided\n",
    "[here](https://codelabs.developers.google.com/codelabs/xgb-caip-e2e#0)\n",
    "and we updated some parts for clarity.\n",
    "\n",
    "This exercise requires a GCP account but don’t worry if you don’t have\n",
    "one. You will be using the same tools in the course assignements and you\n",
    "will get free usage of GCP in those labs. You already saw how to use the\n",
    "tool in the screencast and you will get more hands on with it in next\n",
    "week’s assignments.\n",
    "\n",
    "Alternatively, if you don’t have a Google Cloud Platform (GCP) account\n",
    "yet, you can get a free trial by clicking the `Get Started for Free`\n",
    "button [here](https://cloud.google.com/free). Within 24 hours, you will\n",
    "get 300USD free credits and this exercise will just take 1USD (or less)\n",
    "of that total amount. Just take note of the cleanup instructions\n",
    "mentioned in the last bullet point below to delete all resources you’ve\n",
    "created in the tutorial. That should prevent recurring costs. Once you\n",
    "have it setup and you have your free credits, you can go to the [Cloud\n",
    "Console](https://console.cloud.google.com/) to start the lab.\n",
    "\n",
    "## 1: Overview\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "Click here to expand!\n",
    "</summary>\n",
    "\n",
    "In this lab, you will walk through a complete ML workflow on GCP. From a\n",
    "Cloud AI Platform Notebooks environment, you’ll ingest data from a\n",
    "BigQuery public dataset, build and train an XGBoost model, and deploy\n",
    "the model to AI Platform for prediction.\n",
    "\n",
    "You’ll learn how to:\n",
    "\n",
    "-   Ingest and analyze a BigQuery dataset in AI Platform Notebooks\n",
    "-   Build an XGBoost model\n",
    "-   Deploy the XGBoost model to AI Platform and get predictions\n",
    "-   The total cost to run this lab on Google Cloud is about \\$1.\n",
    "\n",
    "*Tip: It is best to have at least two windows open when going through\n",
    "the instructions in this tutorial: at least one for navigating to the\n",
    "different parts of GCP (e.g. Storage, BigQuery, AI Platform Models) and\n",
    "one for the AI Platform Jupyter Notebook you will open in Step 2 below.*\n",
    "\n",
    "</details>\n",
    "\n",
    "## 2 : Setup Your Environment\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "Click here to expand!\n",
    "</summary>\n",
    "\n",
    "You’ll need a Google Cloud Platform project to run this exercise. If you\n",
    "just enabled a GCP free trial, you should already have a project called\n",
    "‘My First Project’. If not, you can follow the [instructions\n",
    "here](https://cloud.google.com/resource-manager/docs/creating-managing-projects)\n",
    "to create a project.\n",
    "\n",
    "**Step 1**: Enable the Cloud AI Platform Models API\n",
    "\n",
    "Navigate to the [AI Platform\n",
    "Models](https://console.cloud.google.com/ai-platform/models) section of\n",
    "your Cloud Console and click Enable if it isn’t already enabled.\n",
    "\n",
    "<img src='./images/models_api.png' alt='images/models_api'>\n",
    "\n",
    "**Step 2**: Enable the [Compute Engine\n",
    "API](https://console.cloud.google.com/marketplace/details/google/compute.googleapis.com)\n",
    "\n",
    "Navigate to `Compute Engine` and select `Enable` if it isn’t already\n",
    "enabled. You’ll need this to create your notebook instance. (*tip: After\n",
    "clicking Enable and it doesn’t automatically refresh, you can just\n",
    "manually refresh the page after a minute to see if the API has been\n",
    "enabled. It should show “API Enabled”.*\n",
    "\n",
    "**Step 3**: Create an AI Platform Notebooks instance\n",
    "\n",
    "Navigate to [AI Platform\n",
    "Notebooks](https://console.cloud.google.com/mlengine/notebooks/instances)\n",
    "section of your Cloud Console and click *New Instance*. Then select the\n",
    "latest *Python* instance type:\n",
    "\n",
    "<img src='./images/notebook.png' alt='images/notebook.png'>\n",
    "\n",
    "Use the default options and then click *Create*. Once the instance has\n",
    "been created, select *Open JupyterLab*.\n",
    "\n",
    "**Step 4**: Install XGBoost\n",
    "\n",
    "Once your JupyterLab instance has opened, you’ll need to add the XGBoost\n",
    "package.\n",
    "\n",
    "To do this, select Terminal from the launcher:\n",
    "\n",
    "<img src='./images/terminal_launcher.png' alt='images/terminal_launcher.png'>\n",
    "\n",
    "Then run the following to install the latest version of XGBoost\n",
    "supported by AI Platform:\n",
    "\n",
    "    pip3 install xgboost==1.4.2\n",
    "\n",
    "After this completes, close the Terminal (`x` on the upper right) to go\n",
    "back to the Launcher:\n",
    "\n",
    "<img src='./images/close.png' alt='images/close.png'>\n",
    "\n",
    "From there, you can open a `Python 3` Notebook instance. You’re ready to\n",
    "get started in your notebook!\n",
    "\n",
    "**Step 5**: Import Python packages\n",
    "\n",
    "*For the rest of this codelab, run all the code snippets from your\n",
    "Jupyter notebook.*\n",
    "\n",
    "In the first cell of your notebook, add the following imports and run\n",
    "the cell. You can run it by pressing the right arrow button in the top\n",
    "menu or pressing command-enter:\n",
    "\n",
    "    import pandas as pd\n",
    "    import xgboost as xgb\n",
    "    import numpy as np\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.utils import shuffle\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "</details>\n",
    "\n",
    "## 3 : Exploring the BigQuery dataset\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "Click here to expand!\n",
    "</summary>\n",
    "\n",
    "BigQuery has made [many\n",
    "datasets](https://cloud.google.com/bigquery/public-data) publicly\n",
    "available for your exploration. For this lab, we’ll be using the\n",
    "[natality\n",
    "dataset](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=samples&t=natality&page=table&_ga=2.91160473.24786528.1580741008-413280468.1556849151).\n",
    "This contains data on nearly every birth in the US over a 40 year time\n",
    "period, including the birth weight of the child, and demographic\n",
    "information on the baby’s parents. You’ll be using a subset of the\n",
    "features to predict a baby’s birth weight.\n",
    "\n",
    "**Step 1**: Download the BigQuery data to our notebook\n",
    "\n",
    "You’ll be using the Python client library for BigQuery to download the\n",
    "data into a Pandas DataFrame. The original dataset is 21GB and contains\n",
    "123M rows. To keep things simple we’ll only be using 10,000 rows from\n",
    "the dataset.\n",
    "\n",
    "Construct the query and preview the resulting DataFrame with the\n",
    "following code. Here we’re getting 4 features from the original dataset,\n",
    "along with baby weight (the thing our model will predict). The dataset\n",
    "goes back many years but for this model we’ll use only data from after\n",
    "2000:\n",
    "\n",
    "    query=\"\"\"\n",
    "    SELECT\n",
    "      weight_pounds,\n",
    "      is_male,\n",
    "      mother_age,\n",
    "      plurality,\n",
    "      gestation_weeks\n",
    "    FROM\n",
    "      publicdata.samples.natality\n",
    "    WHERE year > 2000\n",
    "    LIMIT 10000\n",
    "    \"\"\"\n",
    "    df = bigquery.Client().query(query).to_dataframe()\n",
    "    df.head()\n",
    "\n",
    "*If you get a `403 Forbidden` error, it means you will need to enable\n",
    "the `BigQuery API` for your account. Search for `BigQuery` in the Search\n",
    "Bar and click `Enable API`. Kindly wait for it to be enabled before\n",
    "re-running the command above.*\n",
    "\n",
    "To get a summary of the numeric features in our dataset, run:\n",
    "\n",
    "    df.describe()\n",
    "\n",
    "This shows the mean, standard deviation, minimum, and other metrics for\n",
    "our numeric columns. Finally, let’s get some data on our boolean column\n",
    "indicating the baby’s gender. We can do this with Pandas’ `value_counts`\n",
    "method:\n",
    "\n",
    "    df['is_male'].value_counts()\n",
    "\n",
    "Looks like the dataset is nearly balanced 50/50 by gender.\n",
    "</details>\n",
    "\n",
    "## 4 : Prepare Data for Training\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "Click here to expand!\n",
    "</summary>\n",
    "\n",
    "In this section, we’ll divide the data into train and test sets to\n",
    "prepare it for training our model.\n",
    "\n",
    "**Step 1**: Extract the label column\n",
    "\n",
    "First drop rows with null values from the dataset and shuffle the data:\n",
    "\n",
    "    df = df.dropna()\n",
    "    df = shuffle(df, random_state=2)\n",
    "\n",
    "Next, extract the label column into a separate variable and create a\n",
    "DataFrame with only our features:\n",
    "\n",
    "    labels = df['weight_pounds']\n",
    "    data = df.drop(columns=['weight_pounds'])\n",
    "\n",
    "Now if you preview our dataset by running `data.head()`, you should see\n",
    "the four features we’ll be using for training.\n",
    "\n",
    "**Step 2**: Convert categorical features to integers\n",
    "\n",
    "Since XGBoost requires all data to be numeric, we’ll need to change how\n",
    "we’re representing the data in the `is_male` column, which is currently\n",
    "True / False strings. We can do that simply by changing the type of that\n",
    "column:\n",
    "\n",
    "    data['is_male'] = data['is_male'].astype(int)\n",
    "\n",
    "**Step 3**: Split data into train and test sets\n",
    "\n",
    "We’ll use Scikit Learn’s `train_test_split` utility which we imported at\n",
    "the beginning of the notebook to split our data into train and test\n",
    "sets:\n",
    "\n",
    "    x,y = data,labels\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y)\n",
    "\n",
    "Now we’re ready to build and train our model!\n",
    "</details>\n",
    "\n",
    "## 5 : A quick XGBoost primer\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "Click here to expand!\n",
    "</summary>\n",
    "\n",
    "[XGBoost](https://github.com/dmlc/xgboost) is a machine learning\n",
    "framework that uses [decision\n",
    "trees](https://en.wikipedia.org/wiki/Decision_tree_learning) and\n",
    "[gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) to\n",
    "build predictive models. It works by ensembling multiple decision trees\n",
    "together based on the score associated with different leaf nodes in a\n",
    "tree.\n",
    "\n",
    "The diagram below is a simplified visualization of an ensemble tree\n",
    "network for a model that evaluates whether or not someone will like a\n",
    "specific computer game (this is from the [XGBoost\n",
    "docs](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)):\n",
    "\n",
    "<img src='./images/xgboost.png' alt='images/xgboost'>\n",
    "\n",
    "Why are we using XGBoost for this model? While traditional neural\n",
    "networks have been shown to perform best on unstructured data like\n",
    "images and text, decision trees often perform extremely well on\n",
    "structured data like the birth weight dataset we’ll be using.\n",
    "</details>\n",
    "\n",
    "## 6 : Build, train, and evaluate an XGBoost model\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "Click here to expand!\n",
    "</summary>\n",
    "\n",
    "**Step 1**: Define and train the XGBoost model Creating a model in\n",
    "XGBoost is simple. We’ll use the `XGBRegressor` class to create the\n",
    "model, and just need to pass the right `objective` parameter for our\n",
    "specific task. Here we’re using a regression model since we’re\n",
    "predicting a numerical value (baby’s weight). If we were instead\n",
    "bucketing our data to determine if a baby weighed more or less than 6\n",
    "pounds, we’d use a classification model.\n",
    "\n",
    "In this case we’ll use `reg:squarederror` as our model’s objective.\n",
    "\n",
    "The following code will create an XGBoost model:\n",
    "\n",
    "    model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror'\n",
    "    )\n",
    "\n",
    "You can train the model with one line of code, calling the fit() method\n",
    "and passing it the training data and labels.\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "**Step 2**: Evaluate your model on test data\n",
    "\n",
    "We can now use our trained model to generate predictions on our test\n",
    "data with the `predict()` function:\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "Let’s see how the model performed on the first 20 values from our test\n",
    "set. Below we’ll print the predicted baby weight along with the actual\n",
    "baby weight for each test example:\n",
    "\n",
    "    for i in range(20):\n",
    "        print('Predicted weight: ', y_pred[i])\n",
    "        print('Actual weight: ', y_test.iloc[i])\n",
    "        print()\n",
    "\n",
    "**Step 3**: Save your model In order to deploy the model, run the\n",
    "following code to save it to a local file:\n",
    "\n",
    "    model.save_model('model.bst')\n",
    "\n",
    "</details>\n",
    "\n",
    "## 7 : Deploy model to Cloud AI Platform\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "Click here to expand!\n",
    "</summary>\n",
    "\n",
    "We’ve got our model working locally, but it would be nice if we could\n",
    "make predictions on it from anywhere (not just this notebook!). In this\n",
    "step we’ll deploy it to the cloud.\n",
    "\n",
    "**Step 1**: Create a Cloud Storage bucket for our model\n",
    "\n",
    "Let’s first define some environment variables that we’ll be using\n",
    "throughout the rest of the tutorial. Fill in the values below with your\n",
    "PROJECT ID, the name of the cloud storage bucket you’d like to create\n",
    "(must be globally unique, you can use the project id as well), and the\n",
    "version name for the first version of your model.\n",
    "\n",
    "*Tip: You can get the Project ID as shown by Laurence in the screencast\n",
    "or by running this command in a cell:\n",
    "`!gcloud config list project --format \"value(core.project)\"`. You can\n",
    "use the result to fill in `<YOUR_PROJECT_ID>` below:*\n",
    "\n",
    "    # Update these to your own GCP project, model, and version names\n",
    "    GCP_PROJECT = '<YOUR_PROJECT_ID>'\n",
    "    MODEL_BUCKET = 'gs://<YOUR_PROJECT_ID>'\n",
    "    VERSION_NAME = 'v1'\n",
    "    MODEL_NAME = 'baby_weight'\n",
    "\n",
    "Now we’re ready to create a storage bucket to store our XGBoost model\n",
    "file. We’ll point Cloud AI Platform at this file when we deploy.\n",
    "\n",
    "Run this `gsutil` command from within your notebook to create a bucket:\n",
    "\n",
    "    !gsutil mb $MODEL_BUCKET\n",
    "\n",
    "**Step 2**: Copy the model file to Cloud Storage\n",
    "\n",
    "Next, we’ll copy our XGBoost saved model file to Cloud Storage. Run the\n",
    "following gsutil command:\n",
    "\n",
    "    !gsutil cp ./model.bst $MODEL_BUCKET\n",
    "\n",
    "*If you get errors about creating buckets, you may need to enable the\n",
    "`Cloud Storage API` before retrying the command above. Just search for\n",
    "`Cloud Storage` using the Search Bar then click `Enable API`.*\n",
    "\n",
    "Head over to the storage browser in your Cloud Console to confirm the\n",
    "file has been copied:\n",
    "\n",
    "<img src='./images/gcs.png' alt='images/gcs'>\n",
    "\n",
    "**Step 3**: Create and deploy the model\n",
    "\n",
    "The following ai-platform gcloud command will create a new model in your\n",
    "project:\n",
    "\n",
    "    !gcloud ai-platform models create $MODEL_NAME --region=us-central1\n",
    "\n",
    "Now it’s time to deploy the model. We can do that with this gcloud\n",
    "command:\n",
    "\n",
    "    !gcloud ai-platform versions create $VERSION_NAME \\\n",
    "    --model=$MODEL_NAME \\\n",
    "    --framework='XGBOOST' \\\n",
    "    --runtime-version=2.5 \\\n",
    "    --origin=$MODEL_BUCKET \\\n",
    "    --python-version=3.7 \\\n",
    "    --project=$GCP_PROJECT \\\n",
    "    --region=us-central1\n",
    "\n",
    "While this is running, check the [models\n",
    "section](https://console.cloud.google.com/ai-platform/models) of your AI\n",
    "Platform console. You should see your new version deploying there:\n",
    "\n",
    "<img src='./images/deploy.png' alt='images/deploy'>\n",
    "\n",
    "When the deploy completes successfully you’ll see a green check mark\n",
    "where the loading spinner is. The deployment can take up to 5 minutes.\n",
    "\n",
    "**Step 4**: Test the deployed model\n",
    "\n",
    "To make sure your deployed model is working, test it out using gcloud to\n",
    "make a prediction. First, save a JSON file with two examples from our\n",
    "test set:\n",
    "\n",
    "    %%writefile predictions.json\n",
    "    [0.0, 33.0, 1.0, 27.0]\n",
    "    [1.0, 26.0, 1.0, 40.0]\n",
    "\n",
    "Test your model by saving the output of the following gcloud command to\n",
    "a variable and printing it:\n",
    "\n",
    "    prediction = !gcloud ai-platform predict --model=$MODEL_NAME --json-instances=predictions.json --version=$VERSION_NAME\n",
    "    print(prediction.s)\n",
    "\n",
    "You should see your model’s prediction in the output. The actual baby\n",
    "weight for these two examples is around 2 and 8 pounds respectively\n",
    "(results may differ slightly because we shuffled our dataset).\n",
    "</details>\n",
    "\n",
    "## 8 : Cleanup\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "Click here to expand!\n",
    "</summary>\n",
    "\n",
    "If you’d like to continue using this notebook, it is recommended that\n",
    "you turn it off when not in use. From the Notebooks UI in your Cloud\n",
    "Console, select the notebook and then select Stop:\n",
    "\n",
    "<img src='./images/cleanup.png' alt='images/cleanup'>\n",
    "\n",
    "If you’d like to delete all resources you’ve created in this lab, simply\n",
    "delete the notebook instance instead of stopping it.\n",
    "\n",
    "Using the Navigation menu in your Cloud Console, browse to\n",
    "`Cloud Storage` and delete both buckets you created to store your model\n",
    "assets. Similarly, you can also go to the dashboard of\n",
    "`AI Platform -> Models` to delete the model manually.\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
