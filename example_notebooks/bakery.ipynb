{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data is from [this](https://www.kaggle.com/competitions/grupo-bimbo-inventory-demand/overview) dataset.\n",
        "\n",
        "The goal is to predict the daily consumer demand for fresh bakery products on the shelves of over 1 milion stores along 45.000 routes across Mexico."
      ],
      "metadata": {
        "id": "aGF05TCiV4qC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Basic Model as a starting point\n",
        "\n",
        "Model selection and model fitting is not our goal here"
      ],
      "metadata": {
        "id": "yBu3g4BLL-9p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEBbdNuDQx_0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "\n",
        "plt.style.use('fivethirtyeight')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "aNiaSMxJkbiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "data_dir = \"/content\"\n",
        "data = pd.read_csv(data_dir / \"train.csv\")\n",
        "clientes = pd.read_csv(data_dir / \"cliente_tabla.csv\")\n",
        "productos = pd.read_csv(data_dir / \"producto_tabla.csv\")\n",
        "town_state = pd.read_csv(data_dir / \"town_state.csv\")"
      ],
      "metadata": {
        "id": "yoZj9pXCkmLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge datasets\n",
        "data = data.merge(clientes, on=\"Cliente_ID\", how=\"left\")\n",
        "data = data.merge(productos, on=\"Producto_ID\", how=\"left\")\n",
        "data = data.merge(town_state, on=\"Agencia_ID\", how=\"left\")\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "kADVRaKck5YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some preprocessing\n",
        "\n",
        "# Define the categorical columns\n",
        "categorical_cols = [\"Agencia_ID\", \"Canal_ID\", \"Ruta_SAK\", \"Cliente_ID\", \"Producto_ID\"]\n",
        "\n",
        "# Define the label encoder\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    le.fit(data[col])\n",
        "    data[col] = le.transform(data[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "num_unique_vals = {col: data[col].nunique() for col in categorical_cols}\n",
        "embedding_sizes = {col: min(50, num_unique_vals[col] // 2) for col in categorical_cols}"
      ],
      "metadata": {
        "id": "q6oW5eqMk6-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into features and target\n",
        "X = data[categorical_cols].values\n",
        "y = data[\"Demanda_uni_equil\"].values\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "eorATqnkk_sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Dataset class\n",
        "class BimboDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = [torch.tensor(X[:, i], dtype=torch.long) for i in range(X.shape[1])]\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return [x[idx] for x in self.X], self.y[idx]\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = BimboDataset(X_train, y_train)\n",
        "val_dataset = BimboDataset(X_val, y_val)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "jMnJL4eKlCSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, embedding_sizes, hidden_size=128):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.embeddings = nn.ModuleList(\n",
        "            [\n",
        "                nn.Embedding(num_unique_vals[col], embedding_sizes[col])\n",
        "                for col in categorical_cols\n",
        "            ])\n",
        "        self.fc1 = nn.Linear(sum(embedding_sizes.values()), hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = [embedding(x_i) for x_i, embedding in zip(x, self.embeddings)]\n",
        "        x = torch.cat(x, dim=-1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x).squeeze(-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define a function to train the modle\n",
        "# Will do it outside the function\n",
        "def train_model(loss_fn, num_epochs=5):\n",
        "    model = SimpleModel(embedding_sizes)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for inputs, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_preds = []\n",
        "        val_targets = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                outputs = model(inputs).squeeze()\n",
        "                loss = loss_fn(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "                val_preds.extend(outputs.tolist())\n",
        "                val_targets.extend(targets.tolist())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        r2 = r2_score(val_targets, val_preds)\n",
        "\n",
        "    return model, np.array(val_preds), np.array(val_targets)"
      ],
      "metadata": {
        "id": "gHJw71DxlIuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_business_metrics(stocking_decisions, actual_demand):\n",
        "\n",
        "    frac_understocks = (stocking_decisions < actual_demand).mean()\n",
        "    total_understocked_amt = (actual_demand - stocking_decisions).clip(0).sum()\n",
        "    frac_overstocks = (stocking_decisions > actual_demand).mean()\n",
        "    total_overstocked_amt = (stocking_decisions - actual_demand).clip(0).sum()\n",
        "\n",
        "    utility = -3 * total_understocked_amt - total_overstocked_amt\n",
        "    mae = mean_absolute_error(actual_demand, stocking_decisions)\n",
        "    mse = mean_squared_error(actual_demand, stocking_decisions)\n",
        "    r2 = r2_score(actual_demand, stocking_decisions)\n",
        "\n",
        "    # add them in a dictionary\n",
        "    metrics = {\n",
        "        'frac_understocks': frac_understocks,\n",
        "        'total_understocked_amt': total_understocked_amt,\n",
        "        'frac_overstocks': frac_overstocks,\n",
        "        'total_overstocked_amt': total_overstocked_amt,\n",
        "        'utility': utility,\n",
        "        'mae': mae,\n",
        "        'mse': mse,\n",
        "        'r2': r2}\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "gZd9eikXlLF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some train-specifics parameters\n",
        "loss_fn = nn.MSELoss()\n",
        "num_epochs = 5\n",
        "model = SimpleModel(embedding_sizes)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)"
      ],
      "metadata": {
        "id": "1a2o4jTDnS-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs).squeeze()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_preds = []\n",
        "    val_targets = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            val_loss += loss.item()\n",
        "            val_preds.extend(outputs.tolist())\n",
        "            val_targets.extend(targets.tolist())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    r2 = r2_score(val_targets, val_preds)\n",
        "\n",
        "\n",
        "# copy the final results\n",
        "mse_model, mse_val_preds, mse_val_targets = model, np.array(val_preds), np.array(val_targets)\n",
        "mse_val_stock = np.ceil(mse_val_preds)"
      ],
      "metadata": {
        "id": "2Wz7dSMpm_8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model results\n",
        "get_business_metrics(mse_val_stock, mse_val_targets)"
      ],
      "metadata": {
        "id": "52gE_IHzndPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try a different decision for re-stocking\n",
        "alternative_stocking_rule = np.ceil(1.5 * mse_val_preds)\n",
        "get_business_metrics(alternative_stocking_rule, mse_val_targets)"
      ],
      "metadata": {
        "id": "f13rkrckoPil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now train using MAE\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs).squeeze()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_preds = []\n",
        "    val_targets = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            val_loss += loss.item()\n",
        "            val_preds.extend(outputs.tolist())\n",
        "            val_targets.extend(targets.tolist())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    r2 = r2_score(val_targets, val_preds)\n",
        "\n",
        "\n",
        "# copy the final results\n",
        "mae_model, mae_val_preds, mae_val_targets = model, np.array(val_preds), np.array(val_targets)\n",
        "mae_val_stock = np.ceil(mae_val_preds)"
      ],
      "metadata": {
        "id": "rYtyXYolotJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_business_metrics(mae_val_stock, mae_val_targets)"
      ],
      "metadata": {
        "id": "yr8vEhQSpF1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets assume understock costs 3€ per unit and overstock costs 1€ per unit"
      ],
      "metadata": {
        "id": "KBeHNGIeqhrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom loss function\n",
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomLoss, self).__init__()\n",
        "\n",
        "    def forward(self, outputs, actual):\n",
        "        diff = outputs - actual\n",
        "        loss = torch.where(outputs > actual, diff, -3 * diff)\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "EYLIs7zIqqdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with the new loss\n",
        "loss_fn = CustomLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs).squeeze()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_preds = []\n",
        "    val_targets = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            val_loss += loss.item()\n",
        "            val_preds.extend(outputs.tolist())\n",
        "            val_targets.extend(targets.tolist())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    r2 = r2_score(val_targets, val_preds)\n",
        "\n",
        "\n",
        "# copy the final results\n",
        "custom_model, custom_val_preds, custom_val_targets = model, np.array(val_preds), np.array(val_targets)\n",
        "custom_val_stock  = np.ceil(custom_val_preds)"
      ],
      "metadata": {
        "id": "rtBAtlI9q0Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then go Streamlit!"
      ],
      "metadata": {
        "id": "yS5o-3oFsBKJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}