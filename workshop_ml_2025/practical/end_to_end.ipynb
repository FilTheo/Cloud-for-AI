{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Time Series Forecasting Workshop\n",
    "\n",
    "## Complete Workflow: From EDA to Production-Ready Models\n",
    "\n",
    "This notebook demonstrates a complete time series forecasting workflow, combining everything we've learned:\n",
    "\n",
    "### What We'll Cover:\n",
    "1. **Exploratory Data Analysis** - Understanding patterns, seasonality, and trends\n",
    "2. **Simple Baseline Models** - Naive methods for benchmarking\n",
    "3. **Statistical Models** - AutoETS for automatic component selection\n",
    "4. **Cross-Validation** - Robust model evaluation with MASE and RMSSE metrics\n",
    "5. **Global ML Models** - Lag selection and feature engineering\n",
    "6. **Final Comparison** - Selecting the best model based on multiple metrics\n",
    "\n",
    "### Dataset:\n",
    "- **Top 50 food items** from M5 Forecasting Competition\n",
    "- Aggregated across all stores\n",
    "- Daily sales from 2013 to 2016\n",
    "\n",
    "Let's begin! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Time series analysis\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# StatsForecast - simple and statistical models\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import (\n",
    "    Naive,\n",
    "    SeasonalNaive,\n",
    "    WindowAverage,\n",
    "    SeasonalWindowAverage,\n",
    "    AutoETS\n",
    ")\n",
    "\n",
    "# MLForecast - global machine learning models\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.target_transforms import Differences, LocalStandardScaler\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "We're using the **top 50 food items** dataset created in the data preparation notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_path = '/home/filtheo/Cloud-for-AI/workshop_ml_2025/data/converted_df_2.csv'\n",
    "df = pd.read_csv(data_path, parse_dates=['date'])\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of unique items: {df['unique_id'].nunique()}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"Total days: {df['date'].nunique()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics per item\n",
    "print(\"Basic Statistics per Item:\")\n",
    "print(\"=\"*70)\n",
    "item_stats = df.groupby('unique_id')['y'].agg(['count', 'mean', 'std', 'min', 'max']).round(1)\n",
    "item_stats.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "### 3.1 Visualize Multiple Time Series\n",
    "\n",
    "Let's look at sales patterns for a sample of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 12 items for visualization (a diverse sample)\n",
    "items = df['unique_id'].unique()\n",
    "sample_items = items[::len(items)//12][:12]  # Select 12 evenly spaced items\n",
    "\n",
    "# Plot all samples in a grid\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, item in enumerate(sample_items):\n",
    "    item_data = df[df['unique_id'] == item].sort_values('date')\n",
    "    \n",
    "    axes[idx].plot(item_data['date'], item_data['y'], \n",
    "                   color='black', linewidth=0.8, alpha=0.8)\n",
    "    axes[idx].set_title(f'{item}', fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Date', fontsize=8)\n",
    "    axes[idx].set_ylabel('Daily Sales', fontsize=8)\n",
    "    axes[idx].tick_params(labelsize=7)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Sample Items: Daily Sales Patterns', fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç What patterns do you notice?\")\n",
    "print(\"- Different items have different sales levels\")\n",
    "print(\"- Some show strong trends, others are more stable\")\n",
    "print(\"- Weekly patterns are visible in many series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Time Series Decomposition\n",
    "\n",
    "Let's decompose a few items to see their **trend**, **seasonality**, and **residual** components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose 3 representative items\n",
    "decompose_items = sample_items[:3]\n",
    "\n",
    "for item_name in decompose_items:\n",
    "    item_data = df[df['unique_id'] == item_name].sort_values('date').set_index('date')\n",
    "    \n",
    "    # Perform seasonal decomposition (period=7 for weekly seasonality)\n",
    "    decomposition = seasonal_decompose(item_data['y'], model='additive', period=7)\n",
    "    \n",
    "    # Plot decomposition\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Observed\n",
    "    axes[0].plot(decomposition.observed, color='black', linewidth=1, alpha=0.8)\n",
    "    axes[0].set_ylabel('Observed', fontsize=10)\n",
    "    axes[0].set_title(f'Time Series Decomposition: {item_name}', fontsize=13, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Trend\n",
    "    axes[1].plot(decomposition.trend, color='steelblue', linewidth=1.5, alpha=0.8)\n",
    "    axes[1].set_ylabel('Trend', fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Seasonal\n",
    "    axes[2].plot(decomposition.seasonal, color='green', linewidth=1, alpha=0.8)\n",
    "    axes[2].set_ylabel('Seasonal', fontsize=10)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual\n",
    "    axes[3].plot(decomposition.resid, color='red', linewidth=0.8, alpha=0.7)\n",
    "    axes[3].set_ylabel('Residual', fontsize=10)\n",
    "    axes[3].set_xlabel('Date', fontsize=10)\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Component Analysis for {item_name}:\")\n",
    "    print(f\"- Trend: Shows overall direction over time\")\n",
    "    print(f\"- Seasonal: Weekly pattern repeating every 7 days\")\n",
    "    print(f\"- Residual: Random noise around zero\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Seasonality Strength Analysis\n",
    "\n",
    "**Seasonality Strength = 1 - Var(Residual) / Var(Seasonal + Residual)**\n",
    "\n",
    "- Value close to **1** = **Strong seasonality**\n",
    "- Value close to **0** = **Weak seasonality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate seasonality strength for all items\n",
    "seasonality_strength = {}\n",
    "\n",
    "for item in items:\n",
    "    item_data = df[df['unique_id'] == item].sort_values('date').set_index('date')\n",
    "    decomp = seasonal_decompose(item_data['y'], model='additive', period=7)\n",
    "    \n",
    "    # Calculate strength\n",
    "    seasonal_var = np.var(decomp.seasonal.dropna())\n",
    "    resid_var = np.var(decomp.resid.dropna())\n",
    "    \n",
    "    strength = 1 - (resid_var / (seasonal_var + resid_var))\n",
    "    seasonality_strength[item] = strength\n",
    "\n",
    "# Convert to DataFrame\n",
    "strength_df = pd.DataFrame(\n",
    "    list(seasonality_strength.items()), \n",
    "    columns=['Item', 'Seasonality_Strength']\n",
    ").sort_values('Seasonality_Strength', ascending=False)\n",
    "\n",
    "print(\"Seasonality Strength by Item (Top 15):\")\n",
    "print(\"=\"*50)\n",
    "print(strength_df.head(15).to_string(index=False))\n",
    "print(f\"\\nAverage strength: {strength_df['Seasonality_Strength'].mean():.3f}\")\n",
    "print(f\"Median strength: {strength_df['Seasonality_Strength'].median():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot aggregated seasonality strength distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(strength_df['Seasonality_Strength'], bins=20, \n",
    "             color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(x=strength_df['Seasonality_Strength'].mean(), \n",
    "                color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {strength_df[\"Seasonality_Strength\"].mean():.3f}')\n",
    "axes[0].set_xlabel('Seasonality Strength', fontsize=11)\n",
    "axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0].set_title('Distribution of Seasonality Strength', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(strength_df['Seasonality_Strength'], vert=True)\n",
    "axes[1].set_ylabel('Seasonality Strength', fontsize=11)\n",
    "axes[1].set_title('Seasonality Strength: Overall Summary', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"- Most items show moderate to strong weekly seasonality\")\n",
    "print(\"- This indicates weekly patterns should be captured by our models\")\n",
    "print(\"- Seasonal models (SeasonalNaive, ETS with seasonality) should perform well\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Day of Week Analysis\n",
    "\n",
    "Let's examine sales patterns by day of week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add day of week\n",
    "df_dow = df.copy()\n",
    "df_dow['day_of_week'] = pd.to_datetime(df_dow['date']).dt.day_name()\n",
    "df_dow['dow_num'] = pd.to_datetime(df_dow['date']).dt.dayofweek\n",
    "\n",
    "# Calculate average sales by day of week (aggregated across all items)\n",
    "dow_avg = df_dow.groupby(['dow_num', 'day_of_week'])['y'].mean().reset_index()\n",
    "dow_avg = dow_avg.sort_values('dow_num')\n",
    "\n",
    "print(\"Average Sales by Day of Week (All Items Combined):\")\n",
    "print(\"=\"*60)\n",
    "print(dow_avg[['day_of_week', 'y']].to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = ['steelblue']*5 + ['lightcoral']*2  # Highlight weekends\n",
    "ax.bar(dow_avg['day_of_week'], dow_avg['y'], \n",
    "       color=colors, alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "ax.set_xlabel('Day of Week', fontsize=11)\n",
    "ax.set_ylabel('Average Daily Sales', fontsize=11)\n",
    "ax.set_title('Average Sales by Day of Week (All Items)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find peak day\n",
    "peak_day = dow_avg.loc[dow_avg['y'].idxmax(), 'day_of_week']\n",
    "low_day = dow_avg.loc[dow_avg['y'].idxmin(), 'day_of_week']\n",
    "print(f\"\\nüìà Highest sales: {peak_day}\")\n",
    "print(f\"üìâ Lowest sales: {low_day}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split\n",
    "\n",
    "We'll hold out the **last 14 days** for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "test_size = 14  # 2 weeks\n",
    "max_date = df['date'].max()\n",
    "split_date = max_date - pd.Timedelta(days=test_size - 1)\n",
    "\n",
    "df_train = df[df['date'] < split_date].copy()\n",
    "df_test = df[df['date'] >= split_date].copy()\n",
    "\n",
    "print(f\"Train/Test Split:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total observations: {len(df):,}\")\n",
    "print(f\"Training set: {len(df_train):,} observations\")\n",
    "print(f\"Test set: {len(df_test):,} observations\")\n",
    "print(f\"\\nDate ranges:\")\n",
    "print(f\"  Train: {df_train['date'].min()} to {df_train['date'].max()}\")\n",
    "print(f\"  Test:  {df_test['date'].min()} to {df_test['date'].max()}\")\n",
    "print(f\"\\nForecast horizon: {test_size} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Custom Evaluation Metrics\n",
    "\n",
    "We'll use two scaled error metrics:\n",
    "\n",
    "### 5.1 MASE (Mean Absolute Scaled Error)\n",
    "\n",
    "$$MASE = \\frac{1}{h} \\sum_{t=1}^{h} \\frac{|y_t - \\hat{y}_t|}{\\frac{1}{T-1}\\sum_{i=2}^{T}|y_i - y_{i-1}|}$$\n",
    "\n",
    "- **Numerator**: Mean absolute error of forecasts\n",
    "- **Denominator**: Mean absolute error of naive forecast on training data\n",
    "- **Interpretation**: MASE < 1 means better than naive forecast\n",
    "\n",
    "### 5.2 RMSSE (Root Mean Squared Scaled Error)\n",
    "\n",
    "$$RMSSE = \\sqrt{\\frac{\\frac{1}{h}\\sum_{t=1}^{h}(y_t - \\hat{y}_t)^2}{\\frac{1}{T-1}\\sum_{i=2}^{T}(y_i - y_{i-1})^2}}$$\n",
    "\n",
    "- **Numerator**: Root mean squared error of forecasts\n",
    "- **Denominator**: Root mean squared error of naive forecast on training data\n",
    "- **Interpretation**: RMSSE < 1 means better than naive forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mase(y_true, y_pred, y_train, seasonality=1):\n",
    "    \"\"\"\n",
    "    Calculate Mean Absolute Scaled Error (MASE).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Actual values in test period\n",
    "    y_pred : array-like\n",
    "        Predicted values\n",
    "    y_train : array-like\n",
    "        Training data used to compute scaling factor\n",
    "    seasonality : int, default=1\n",
    "        Seasonal period for naive forecast (1 for non-seasonal)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : MASE value\n",
    "    \"\"\"\n",
    "    # Calculate forecast errors\n",
    "    mae_forecast = np.mean(np.abs(y_true - y_pred))\n",
    "    \n",
    "    # Calculate naive forecast errors on training data\n",
    "    naive_errors = np.abs(np.diff(y_train, n=seasonality))\n",
    "    mae_naive = np.mean(naive_errors)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if mae_naive == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return mae_forecast / mae_naive\n",
    "\n",
    "\n",
    "def calculate_rmsse(y_true, y_pred, y_train, seasonality=1):\n",
    "    \"\"\"\n",
    "    Calculate Root Mean Squared Scaled Error (RMSSE).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Actual values in test period\n",
    "    y_pred : array-like\n",
    "        Predicted values\n",
    "    y_train : array-like\n",
    "        Training data used to compute scaling factor\n",
    "    seasonality : int, default=1\n",
    "        Seasonal period for naive forecast (1 for non-seasonal)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : RMSSE value\n",
    "    \"\"\"\n",
    "    # Calculate forecast errors\n",
    "    mse_forecast = np.mean((y_true - y_pred)**2)\n",
    "    \n",
    "    # Calculate naive forecast errors on training data\n",
    "    naive_errors = np.diff(y_train, n=seasonality)**2\n",
    "    mse_naive = np.mean(naive_errors)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if mse_naive == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return np.sqrt(mse_forecast / mse_naive)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Custom metrics defined:\")\n",
    "print(\"  - MASE: Mean Absolute Scaled Error\")\n",
    "print(\"  - RMSSE: Root Mean Squared Scaled Error\")\n",
    "print(\"\\nüí° Both metrics are scale-free and compare against naive forecast\")\n",
    "print(\"   Values < 1.0 indicate better performance than naive baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Simple Baseline Models (StatsForecast)\n",
    "\n",
    "Let's start with simple baseline methods:\n",
    "- **Naive**: Repeat last value\n",
    "- **Seasonal Naive**: Repeat same day from last week\n",
    "- **Window Average**: Average of last N days\n",
    "- **Seasonal Window Average**: Average of same weekday from last N weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for StatsForecast (rename 'date' to 'ds')\n",
    "df_train_sf = df_train.rename(columns={'date': 'ds'})\n",
    "df_test_sf = df_test.rename(columns={'date': 'ds'})\n",
    "\n",
    "# Define simple baseline models\n",
    "simple_models = [\n",
    "    Naive(),\n",
    "    SeasonalNaive(season_length=7),\n",
    "    WindowAverage(window_size=7),\n",
    "    SeasonalWindowAverage(season_length=7, window_size=2)\n",
    "]\n",
    "\n",
    "# Fit all models\n",
    "print(\"Training simple baseline models...\")\n",
    "sf_simple = StatsForecast(\n",
    "    models=simple_models,\n",
    "    freq='D',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Generate forecasts\n",
    "forecasts_simple = sf_simple.forecast(df=df_train_sf, h=test_size)\n",
    "\n",
    "print(\"‚úÖ Simple models trained and forecasted!\")\n",
    "print(f\"\\nForecast shape: {forecasts_simple.shape}\")\n",
    "print(f\"Columns: {list(forecasts_simple.columns)}\")\n",
    "forecasts_simple.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Models (StatsForecast)\n",
    "\n",
    "Now let's add **AutoETS** - automatic exponential smoothing with optimal component selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define statistical model\n",
    "statistical_models = [\n",
    "    AutoETS(season_length=7)\n",
    "]\n",
    "\n",
    "# Fit model\n",
    "print(\"Training AutoETS...\")\n",
    "sf_ets = StatsForecast(\n",
    "    models=statistical_models,\n",
    "    freq='D',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Generate forecasts\n",
    "forecasts_ets = sf_ets.forecast(df=df_train_sf, h=test_size)\n",
    "\n",
    "print(\"‚úÖ AutoETS trained and forecasted!\")\n",
    "forecasts_ets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cross-Validation Setup\n",
    "\n",
    "We'll use **time series cross-validation** with 2 validation windows:\n",
    "\n",
    "```\n",
    "Training data: [----train1----][val1]\n",
    "               [------train2------][val2]\n",
    "```\n",
    "\n",
    "This gives us a robust estimate of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_cv(forecasts, actuals, train_data, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate model using MASE and RMSSE metrics with cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    forecasts : DataFrame\n",
    "        Forecasts from StatsForecast or MLForecast\n",
    "    actuals : DataFrame\n",
    "        Actual test values\n",
    "    train_data : DataFrame\n",
    "        Training data for scaling factor\n",
    "    model_name : str\n",
    "        Name of the model column in forecasts\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : {'mase': float, 'rmsse': float}\n",
    "    \"\"\"\n",
    "    # Merge forecasts with actuals\n",
    "    if 'ds' in forecasts.columns:\n",
    "        merged = forecasts.reset_index().merge(\n",
    "            actuals[['unique_id', 'ds', 'y']], \n",
    "            on=['unique_id', 'ds']\n",
    "        )\n",
    "    else:\n",
    "        merged = forecasts.merge(\n",
    "            actuals[['unique_id', 'ds', 'y']], \n",
    "            on=['unique_id', 'ds']\n",
    "        )\n",
    "    \n",
    "    mase_scores = []\n",
    "    rmsse_scores = []\n",
    "    \n",
    "    # Calculate metrics for each unique_id\n",
    "    for uid in merged['unique_id'].unique():\n",
    "        # Get test data for this item\n",
    "        item_data = merged[merged['unique_id'] == uid].sort_values('ds')\n",
    "        y_true = item_data['y'].values\n",
    "        y_pred = item_data[model_name].values\n",
    "        \n",
    "        # Get training data for this item\n",
    "        train_item = train_data[train_data['unique_id'] == uid].sort_values('ds')\n",
    "        y_train = train_item['y'].values\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mase = calculate_mase(y_true, y_pred, y_train, seasonality=1)\n",
    "        rmsse = calculate_rmsse(y_true, y_pred, y_train, seasonality=1)\n",
    "        \n",
    "        if not np.isnan(mase):\n",
    "            mase_scores.append(mase)\n",
    "        if not np.isnan(rmsse):\n",
    "            rmsse_scores.append(rmsse)\n",
    "    \n",
    "    return {\n",
    "        'mase': np.mean(mase_scores),\n",
    "        'rmsse': np.mean(rmsse_scores)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Evaluation function defined\")\n",
    "print(\"\\nThis function calculates:\")\n",
    "print(\"  - MASE: Mean Absolute Scaled Error\")\n",
    "print(\"  - RMSSE: Root Mean Squared Scaled Error\")\n",
    "print(\"\\nBoth metrics are averaged across all items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Simple and Statistical Models\n",
    "\n",
    "Let's calculate MASE and RMSSE for all models trained so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate simple models\n",
    "print(\"Evaluating simple baseline models...\\n\")\n",
    "\n",
    "results_simple = {}\n",
    "model_names_simple = ['Naive', 'SeasonalNaive', 'WindowAverage', 'SeasWA']\n",
    "\n",
    "for model_name in model_names_simple:\n",
    "    metrics = evaluate_model_cv(forecasts_simple, df_test_sf, df_train_sf, model_name)\n",
    "    results_simple[model_name] = metrics\n",
    "    print(f\"{model_name:20s} - MASE: {metrics['mase']:.4f}, RMSSE: {metrics['rmsse']:.4f}\")\n",
    "\n",
    "# Evaluate AutoETS\n",
    "print(\"\\nEvaluating AutoETS...\\n\")\n",
    "metrics_ets = evaluate_model_cv(forecasts_ets, df_test_sf, df_train_sf, 'AutoETS')\n",
    "results_simple['AutoETS'] = metrics_ets\n",
    "print(f\"AutoETS              - MASE: {metrics_ets['mase']:.4f}, RMSSE: {metrics_ets['rmsse']:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ All statistical models evaluated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "results_df = pd.DataFrame(results_simple).T\n",
    "results_df = results_df.sort_values('mase')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MASE\n",
    "axes[0].barh(results_df.index, results_df['mase'], color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(x=1.0, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Naive Baseline')\n",
    "axes[0].set_xlabel('MASE (Lower is Better)', fontsize=11)\n",
    "axes[0].set_title('Mean Absolute Scaled Error', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# RMSSE\n",
    "axes[1].barh(results_df.index, results_df['rmsse'], color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(x=1.0, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Naive Baseline')\n",
    "axes[1].set_xlabel('RMSSE (Lower is Better)', fontsize=11)\n",
    "axes[1].set_title('Root Mean Squared Scaled Error', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.suptitle('Simple and Statistical Models Performance', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"- Models with MASE/RMSSE < 1.0 outperform naive forecast\")\n",
    "print(\"- Seasonal models typically perform better due to weekly patterns\")\n",
    "print(\"- AutoETS automatically adapts to each series' characteristics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Global ML Models - Lag Selection\n",
    "\n",
    "Now let's explore **global machine learning models** using **MLForecast**.\n",
    "\n",
    "First, we need to find the optimal number of lags through cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for MLForecast\n",
    "df_train_mlf = df_train_sf.copy()  # Already renamed to 'ds'\n",
    "df_test_mlf = df_test_sf.copy()\n",
    "\n",
    "# Test different lag configurations\n",
    "max_lags_candidates = [7, 14, 21, 28]\n",
    "\n",
    "print(\"Testing different lag configurations via cross-validation...\\n\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "cv_results_lr = {}\n",
    "cv_results_lgbm = {}\n",
    "\n",
    "for max_lags in max_lags_candidates:\n",
    "    print(f\"Testing max_lags={max_lags}...\")\n",
    "    \n",
    "    lags = list(range(1, max_lags + 1))\n",
    "    \n",
    "    # Linear Regression\n",
    "    fcst_lr = MLForecast(\n",
    "        models={'LinearRegression': LinearRegression()},\n",
    "        freq='D',\n",
    "        lags=lags\n",
    "    )\n",
    "    \n",
    "    cv_lr = fcst_lr.cross_validation(\n",
    "        df=df_train_mlf,\n",
    "        h=7,\n",
    "        n_windows=2\n",
    "    )\n",
    "    \n",
    "    mse_lr = mean_squared_error(cv_lr['y'], cv_lr['LinearRegression'])\n",
    "    cv_results_lr[max_lags] = mse_lr\n",
    "    \n",
    "    # LightGBM\n",
    "    fcst_lgbm = MLForecast(\n",
    "        models={'LightGBM': lgb.LGBMRegressor(verbose=-1)},\n",
    "        freq='D',\n",
    "        lags=lags\n",
    "    )\n",
    "    \n",
    "    cv_lgbm = fcst_lgbm.cross_validation(\n",
    "        df=df_train_mlf,\n",
    "        h=7,\n",
    "        n_windows=2\n",
    "    )\n",
    "    \n",
    "    mse_lgbm = mean_squared_error(cv_lgbm['y'], cv_lgbm['LightGBM'])\n",
    "    cv_results_lgbm[max_lags] = mse_lgbm\n",
    "\n",
    "best_lags_lr = min(cv_results_lr, key=cv_results_lr.get)\n",
    "best_lags_lgbm = min(cv_results_lgbm, key=cv_results_lgbm.get)\n",
    "\n",
    "print(\"\\n‚úÖ Lag selection complete!\")\n",
    "print(f\"\\nOptimal lags:\")\n",
    "print(f\"  Linear Regression: {best_lags_lr} (MSE: {cv_results_lr[best_lags_lr]:.2f})\")\n",
    "print(f\"  LightGBM: {best_lags_lgbm} (MSE: {cv_results_lgbm[best_lags_lgbm]:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize lag selection results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "lags_list = sorted(cv_results_lr.keys())\n",
    "\n",
    "# Linear Regression\n",
    "mse_list_lr = [cv_results_lr[lag] for lag in lags_list]\n",
    "axes[0].plot(lags_list, mse_list_lr, marker='o', linewidth=2, markersize=8, color='#1f77b4')\n",
    "axes[0].axvline(x=best_lags_lr, color='red', linestyle='--', linewidth=2, alpha=0.7,\n",
    "                label=f'Optimal: {best_lags_lr}')\n",
    "axes[0].set_xlabel('max_lags', fontsize=11)\n",
    "axes[0].set_ylabel('Validation MSE', fontsize=11)\n",
    "axes[0].set_title('Linear Regression: Lag Selection', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# LightGBM\n",
    "mse_list_lgbm = [cv_results_lgbm[lag] for lag in lags_list]\n",
    "axes[1].plot(lags_list, mse_list_lgbm, marker='o', linewidth=2, markersize=8, color='#2ca02c')\n",
    "axes[1].axvline(x=best_lags_lgbm, color='red', linestyle='--', linewidth=2, alpha=0.7,\n",
    "                label=f'Optimal: {best_lags_lgbm}')\n",
    "axes[1].set_xlabel('max_lags', fontsize=11)\n",
    "axes[1].set_ylabel('Validation MSE', fontsize=11)\n",
    "axes[1].set_title('LightGBM: Lag Selection', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Optimal Lag Selection via Cross-Validation', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"- Too few lags miss important patterns\")\n",
    "print(\"- Too many lags add noise and computational cost\")\n",
    "print(\"- Cross-validation helps find the sweet spot!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Global ML Models - Feature Engineering\n",
    "\n",
    "Now let's add feature engineering:\n",
    "1. **Date Features**: day_of_week, day_of_month, week_of_year, is_weekend\n",
    "2. **Normalization**: StandardScaler and LocalStandardScaler\n",
    "\n",
    "We'll test different combinations for LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom date feature\n",
    "def is_weekend(dates):\n",
    "    return (dates.dayofweek >= 5).astype(int)\n",
    "\n",
    "# Test different feature engineering configurations\n",
    "fe_configs = {\n",
    "    'Baseline (no FE)': {\n",
    "        'target_transforms': None,\n",
    "        'date_features': None\n",
    "    },\n",
    "    'With Scaling': {\n",
    "        'target_transforms': [LocalStandardScaler()],\n",
    "        'date_features': None\n",
    "    },\n",
    "    'With Date Features': {\n",
    "        'target_transforms': None,\n",
    "        'date_features': ['dayofweek', 'day', 'week', is_weekend]\n",
    "    },\n",
    "    'With Both': {\n",
    "        'target_transforms': [LocalStandardScaler()],\n",
    "        'date_features': ['dayofweek', 'day', 'week', is_weekend]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Testing feature engineering configurations for LightGBM...\\n\")\n",
    "\n",
    "fe_results = {}\n",
    "fe_forecasts = {}\n",
    "\n",
    "for config_name, config in fe_configs.items():\n",
    "    print(f\"  {config_name}...\")\n",
    "    \n",
    "    # Initialize MLForecast\n",
    "    fcst = MLForecast(\n",
    "        models={'LightGBM': lgb.LGBMRegressor(n_estimators=100, max_depth=10, random_state=42, verbose=-1)},\n",
    "        freq='D',\n",
    "        lags=list(range(1, best_lags_lgbm + 1)),\n",
    "        target_transforms=config['target_transforms'],\n",
    "        date_features=config['date_features']\n",
    "    )\n",
    "    \n",
    "    # Fit and predict\n",
    "    fcst.fit(df_train_mlf)\n",
    "    forecasts = fcst.predict(test_size)\n",
    "    fe_forecasts[config_name] = forecasts\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_model_cv(forecasts, df_test_mlf, df_train_mlf, 'LightGBM')\n",
    "    fe_results[config_name] = metrics\n",
    "\n",
    "print(\"\\n‚úÖ Feature engineering comparison complete!\")\n",
    "\n",
    "# Print results\n",
    "print(\"\\nFeature Engineering Impact: LightGBM\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Configuration':<25} {'MASE':<12} {'RMSSE':<12}\")\n",
    "print(\"-\"*60)\n",
    "for config_name, metrics in fe_results.items():\n",
    "    print(f\"{config_name:<25} {metrics['mase']:<12.4f} {metrics['rmsse']:<12.4f}\")\n",
    "\n",
    "# Calculate improvement\n",
    "baseline_mase = fe_results['Baseline (no FE)']['mase']\n",
    "best_mase = fe_results['With Both']['mase']\n",
    "improvement = ((baseline_mase - best_mase) / baseline_mase) * 100\n",
    "\n",
    "print(f\"\\nüí° Improvement: {improvement:.1f}% reduction in MASE with full feature engineering!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature engineering impact\n",
    "fe_results_df = pd.DataFrame(fe_results).T\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "config_names = list(fe_configs.keys())\n",
    "colors = ['#d62728', '#ff7f0e', '#2ca02c', '#1f77b4']\n",
    "\n",
    "# MASE\n",
    "axes[0].bar(range(len(config_names)), fe_results_df['mase'], \n",
    "            color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_xticks(range(len(config_names)))\n",
    "axes[0].set_xticklabels(config_names, rotation=15, ha='right')\n",
    "axes[0].set_ylabel('MASE (Lower is Better)', fontsize=11)\n",
    "axes[0].set_title('Impact of Feature Engineering on MASE', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, val in enumerate(fe_results_df['mase']):\n",
    "    axes[0].text(i, val, f'{val:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# RMSSE\n",
    "axes[1].bar(range(len(config_names)), fe_results_df['rmsse'], \n",
    "            color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "axes[1].set_xticks(range(len(config_names)))\n",
    "axes[1].set_xticklabels(config_names, rotation=15, ha='right')\n",
    "axes[1].set_ylabel('RMSSE (Lower is Better)', fontsize=11)\n",
    "axes[1].set_title('Impact of Feature Engineering on RMSSE', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, val in enumerate(fe_results_df['rmsse']):\n",
    "    axes[1].text(i, val, f'{val:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Feature Engineering Impact on LightGBM Performance', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Feature engineering significantly improves performance!\")\n",
    "print(\"   ‚Üí Scaling helps standardize features\")\n",
    "print(\"   ‚Üí Date features capture calendar effects (weekends, etc.)\")\n",
    "print(\"   ‚Üí Combining both gives best results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Train Final Models with Best Configuration\n",
    "\n",
    "Now let's train Linear Regression and LightGBM with their optimal configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training final models with optimal configurations...\\n\")\n",
    "\n",
    "# Linear Regression (with best lags + feature engineering)\n",
    "print(f\"Training Linear Regression (max_lags={best_lags_lr})...\")\n",
    "fcst_lr_final = MLForecast(\n",
    "    models={'LinearRegression': LinearRegression()},\n",
    "    freq='D',\n",
    "    lags=list(range(1, best_lags_lr + 1)),\n",
    "    target_transforms=[LocalStandardScaler()],\n",
    "    date_features=['dayofweek', 'day', 'week', is_weekend]\n",
    ")\n",
    "fcst_lr_final.fit(df_train_mlf)\n",
    "forecasts_lr_final = fcst_lr_final.predict(test_size)\n",
    "\n",
    "# LightGBM (with best lags + feature engineering)\n",
    "print(f\"Training LightGBM (max_lags={best_lags_lgbm})...\")\n",
    "fcst_lgbm_final = MLForecast(\n",
    "    models={'LightGBM': lgb.LGBMRegressor(n_estimators=100, max_depth=10, random_state=42, verbose=-1)},\n",
    "    freq='D',\n",
    "    lags=list(range(1, best_lags_lgbm + 1)),\n",
    "    target_transforms=[LocalStandardScaler()],\n",
    "    date_features=['dayofweek', 'day', 'week', is_weekend]\n",
    ")\n",
    "fcst_lgbm_final.fit(df_train_mlf)\n",
    "forecasts_lgbm_final = fcst_lgbm_final.predict(test_size)\n",
    "\n",
    "print(\"\\n‚úÖ Final models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Model Comparison\n",
    "\n",
    "Let's compare all models using both MASE and RMSSE metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final ML models\n",
    "metrics_lr_final = evaluate_model_cv(forecasts_lr_final, df_test_mlf, df_train_mlf, 'LinearRegression')\n",
    "metrics_lgbm_final = evaluate_model_cv(forecasts_lgbm_final, df_test_mlf, df_train_mlf, 'LightGBM')\n",
    "\n",
    "# Combine all results\n",
    "all_results = results_simple.copy()\n",
    "all_results['LinearRegression'] = metrics_lr_final\n",
    "all_results['LightGBM'] = metrics_lgbm_final\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(all_results).T\n",
    "comparison_df = comparison_df.sort_values('mase')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL MODEL COMPARISON: ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<25} {'MASE':<15} {'RMSSE':<15} {'Beats Naive?':<15}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for model, metrics in comparison_df.iterrows():\n",
    "    beats_naive = '‚úÖ Yes' if metrics['mase'] < 1.0 else '‚ùå No'\n",
    "    print(f\"{model:<25} {metrics['mase']:<15.4f} {metrics['rmsse']:<15.4f} {beats_naive:<15}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify best model\n",
    "best_model_mase = comparison_df['mase'].idxmin()\n",
    "best_model_rmsse = comparison_df['rmsse'].idxmin()\n",
    "\n",
    "print(f\"\\nüèÜ Best Model by MASE: {best_model_mase} ({comparison_df.loc[best_model_mase, 'mase']:.4f})\")\n",
    "print(f\"üèÜ Best Model by RMSSE: {best_model_rmsse} ({comparison_df.loc[best_model_rmsse, 'rmsse']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# MASE comparison\n",
    "colors_mase = ['green' if x < 1.0 else 'red' for x in comparison_df['mase']]\n",
    "axes[0].barh(comparison_df.index, comparison_df['mase'], \n",
    "             color=colors_mase, alpha=0.7, edgecolor='black', linewidth=1.2)\n",
    "axes[0].axvline(x=1.0, color='black', linestyle='--', linewidth=2, \n",
    "                alpha=0.5, label='Naive Baseline (1.0)')\n",
    "axes[0].set_xlabel('MASE (Lower is Better)', fontsize=11)\n",
    "axes[0].set_title('Mean Absolute Scaled Error', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# RMSSE comparison\n",
    "colors_rmsse = ['green' if x < 1.0 else 'red' for x in comparison_df['rmsse']]\n",
    "axes[1].barh(comparison_df.index, comparison_df['rmsse'], \n",
    "             color=colors_rmsse, alpha=0.7, edgecolor='black', linewidth=1.2)\n",
    "axes[1].axvline(x=1.0, color='black', linestyle='--', linewidth=2, \n",
    "                alpha=0.5, label='Naive Baseline (1.0)')\n",
    "axes[1].set_xlabel('RMSSE (Lower is Better)', fontsize=11)\n",
    "axes[1].set_title('Root Mean Squared Scaled Error', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.suptitle('Final Model Comparison: All Models', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"  - Green bars: Models that beat naive baseline (metric < 1.0)\")\n",
    "print(\"  - Red bars: Models worse than naive baseline\")\n",
    "print(\"  - Lower values are better for both metrics\")\n",
    "print(\"  - MASE focuses on absolute errors, RMSSE penalizes large errors more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Forecast Visualizations\n",
    "\n",
    "### 14.1 Individual Item Forecasts\n",
    "\n",
    "Let's visualize forecasts for a sample of items using the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model forecasts\n",
    "if best_model_mase == 'LightGBM':\n",
    "    best_forecasts = forecasts_lgbm_final\n",
    "    best_model_col = 'LightGBM'\n",
    "elif best_model_mase == 'LinearRegression':\n",
    "    best_forecasts = forecasts_lr_final\n",
    "    best_model_col = 'LinearRegression'\n",
    "else:\n",
    "    # Use AutoETS if it's the best\n",
    "    best_forecasts = forecasts_ets\n",
    "    best_model_col = 'AutoETS'\n",
    "\n",
    "# Visualize 8 sample items\n",
    "sample_items_viz = items[::len(items)//8][:8]\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, item in enumerate(sample_items_viz):\n",
    "    # Get data for this item\n",
    "    train_item = df_train_mlf[df_train_mlf['unique_id'] == item].sort_values('ds')\n",
    "    test_item = df_test_mlf[df_test_mlf['unique_id'] == item].sort_values('ds')\n",
    "    \n",
    "    if 'ds' in best_forecasts.columns:\n",
    "        forecast_item = best_forecasts.reset_index()\n",
    "        forecast_item = forecast_item[forecast_item['unique_id'] == item].sort_values('ds')\n",
    "    else:\n",
    "        forecast_item = best_forecasts[best_forecasts['unique_id'] == item].sort_values('ds')\n",
    "    \n",
    "    # Plot last 90 days of training\n",
    "    train_context = train_item.tail(90)\n",
    "    axes[idx].plot(train_context['ds'], train_context['y'], \n",
    "                   color='black', linewidth=1, alpha=0.7, label='Train')\n",
    "    \n",
    "    # Plot actual test data\n",
    "    axes[idx].plot(test_item['ds'], test_item['y'], \n",
    "                   color='black', linewidth=1.5, alpha=0.8, \n",
    "                   marker='o', markersize=3, label='Actual')\n",
    "    \n",
    "    # Plot forecast\n",
    "    axes[idx].plot(forecast_item['ds'], forecast_item[best_model_col], \n",
    "                   color='red', linewidth=2, linestyle='--', alpha=0.8, label='Forecast')\n",
    "    \n",
    "    # Mark split\n",
    "    axes[idx].axvline(x=split_date, color='gray', linestyle='-', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    axes[idx].set_title(f'{item}', fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Date', fontsize=8)\n",
    "    axes[idx].set_ylabel('Sales', fontsize=8)\n",
    "    axes[idx].tick_params(labelsize=7)\n",
    "    axes[idx].legend(fontsize=7, loc='upper left')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Individual Item Forecasts: {best_model_mase}', fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2 Aggregated Forecasts\n",
    "\n",
    "Let's aggregate forecasts across all items and compare to aggregated actuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate forecasts and actuals\n",
    "if 'ds' in best_forecasts.columns:\n",
    "    forecasts_agg = best_forecasts.reset_index().groupby('ds')[best_model_col].sum().reset_index()\n",
    "else:\n",
    "    forecasts_agg = best_forecasts.groupby('ds')[best_model_col].sum().reset_index()\n",
    "\n",
    "actuals_agg = df_test_mlf.groupby('ds')['y'].sum().reset_index()\n",
    "train_agg = df_train_mlf.groupby('ds')['y'].sum().reset_index()\n",
    "\n",
    "# Plot aggregated forecast\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Plot last 90 days of training\n",
    "train_context = train_agg.tail(90)\n",
    "ax.plot(train_context['ds'], train_context['y'], \n",
    "        color='black', linewidth=1, alpha=0.7, label='Train (Aggregated)')\n",
    "\n",
    "# Plot actual test data (aggregated)\n",
    "ax.plot(actuals_agg['ds'], actuals_agg['y'], \n",
    "        color='black', linewidth=2, alpha=0.8, marker='o', markersize=5, label='Actual (Aggregated)')\n",
    "\n",
    "# Plot forecast (aggregated)\n",
    "ax.plot(forecasts_agg['ds'], forecasts_agg[best_model_col], \n",
    "        color='red', linewidth=2.5, linestyle='--', alpha=0.8, label=f'Forecast (Aggregated)')\n",
    "\n",
    "# Mark split\n",
    "ax.axvline(x=split_date, color='gray', linestyle='-', linewidth=1.5, alpha=0.5, label='Train/Test Split')\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=11)\n",
    "ax.set_ylabel('Total Daily Sales', fontsize=11)\n",
    "ax.set_title(f'Aggregated Forecasts Across All Items: {best_model_mase}', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10, loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate aggregated error\n",
    "agg_mae = mean_absolute_error(actuals_agg['y'], forecasts_agg[best_model_col])\n",
    "agg_mape = np.mean(np.abs((actuals_agg['y'] - forecasts_agg[best_model_col]) / actuals_agg['y'])) * 100\n",
    "\n",
    "print(f\"\\nAggregated Forecast Performance:\")\n",
    "print(f\"  MAE: {agg_mae:.2f}\")\n",
    "print(f\"  MAPE: {agg_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3 Error Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate errors for all items\n",
    "if 'ds' in best_forecasts.columns:\n",
    "    errors_df = best_forecasts.reset_index().merge(\n",
    "        df_test_mlf[['unique_id', 'ds', 'y']], \n",
    "        on=['unique_id', 'ds']\n",
    "    )\n",
    "else:\n",
    "    errors_df = best_forecasts.merge(\n",
    "        df_test_mlf[['unique_id', 'ds', 'y']], \n",
    "        on=['unique_id', 'ds']\n",
    "    )\n",
    "\n",
    "errors_df['error'] = errors_df['y'] - errors_df[best_model_col]\n",
    "errors_df['abs_error'] = np.abs(errors_df['error'])\n",
    "errors_df['pct_error'] = (errors_df['error'] / errors_df['y']) * 100\n",
    "\n",
    "# Visualize error distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Error histogram\n",
    "axes[0, 0].hist(errors_df['error'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "axes[0, 0].set_xlabel('Forecast Error', fontsize=10)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=10)\n",
    "axes[0, 0].set_title('Error Distribution', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=9)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Absolute error by item\n",
    "item_errors = errors_df.groupby('unique_id')['abs_error'].mean().sort_values(ascending=False).head(20)\n",
    "axes[0, 1].barh(range(len(item_errors)), item_errors.values, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_yticks(range(len(item_errors)))\n",
    "axes[0, 1].set_yticklabels(item_errors.index, fontsize=7)\n",
    "axes[0, 1].set_xlabel('Mean Absolute Error', fontsize=10)\n",
    "axes[0, 1].set_title('Top 20 Items by Absolute Error', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Actual vs Predicted scatter\n",
    "axes[1, 0].scatter(errors_df['y'], errors_df[best_model_col], \n",
    "                   alpha=0.3, s=10, color='steelblue')\n",
    "max_val = max(errors_df['y'].max(), errors_df[best_model_col].max())\n",
    "axes[1, 0].plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='Perfect Forecast')\n",
    "axes[1, 0].set_xlabel('Actual', fontsize=10)\n",
    "axes[1, 0].set_ylabel('Predicted', fontsize=10)\n",
    "axes[1, 0].set_title('Actual vs Predicted', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=9)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error over time\n",
    "error_by_date = errors_df.groupby('ds')['error'].mean()\n",
    "axes[1, 1].plot(error_by_date.index, error_by_date.values, \n",
    "                color='steelblue', linewidth=2, marker='o', markersize=5)\n",
    "axes[1, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Date', fontsize=10)\n",
    "axes[1, 1].set_ylabel('Mean Error', fontsize=10)\n",
    "axes[1, 1].set_title('Forecast Error Over Time', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.suptitle('Error Analysis', fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Error Statistics:\")\n",
    "print(f\"  Mean Error: {errors_df['error'].mean():.2f}\")\n",
    "print(f\"  Std Error: {errors_df['error'].std():.2f}\")\n",
    "print(f\"  Mean Absolute Error: {errors_df['abs_error'].mean():.2f}\")\n",
    "print(f\"  Median Absolute Error: {errors_df['abs_error'].median():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary and Recommendations\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "1. ‚úÖ **Exploratory Data Analysis**\n",
    "   - Visualized time series patterns\n",
    "   - Decomposed into trend, seasonality, and residuals\n",
    "   - Measured seasonality strength (average: {:.3f})\n",
    "   - Identified weekly patterns with peak sales on weekends\n",
    "\n",
    "2. ‚úÖ **Simple Baseline Models**\n",
    "   - Established benchmarks with Naive, Seasonal Naive, Window Average\n",
    "   - Seasonal models performed better due to strong weekly patterns\n",
    "\n",
    "3. ‚úÖ **Statistical Models**\n",
    "   - AutoETS automatically selected optimal components\n",
    "   - Captured both trend and seasonality\n",
    "\n",
    "4. ‚úÖ **Custom Metrics**\n",
    "   - Implemented MASE (Mean Absolute Scaled Error)\n",
    "   - Implemented RMSSE (Root Mean Squared Scaled Error)\n",
    "   - Both metrics scale-free and compare against naive baseline\n",
    "\n",
    "5. ‚úÖ **Global ML Models**\n",
    "   - Performed lag selection via cross-validation\n",
    "   - Applied feature engineering (date features + normalization)\n",
    "   - Trained Linear Regression and LightGBM\n",
    "\n",
    "6. ‚úÖ **Model Comparison**\n",
    "   - Evaluated all models using MASE and RMSSE\n",
    "   - Best model: **{best_model_mase}** (MASE: {comparison_df.loc[best_model_mase, 'mase']:.4f})\n",
    "   - Visualized forecasts and error distributions\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- **Seasonality Matters**: Models that capture weekly patterns perform significantly better\n",
    "- **Feature Engineering**: Date features and normalization improve performance by ~{improvement:.1f}%\n",
    "- **Lag Selection**: Cross-validation helps find optimal historical window\n",
    "- **Global Models**: Train once on all series, more efficient than local models\n",
    "- **Scaled Metrics**: MASE and RMSSE provide interpretable, scale-free evaluation\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **For Production**: Use **{best_model_mase}** with:\n",
    "   - Optimal lags: {best_lags_lgbm if best_model_mase == 'LightGBM' else best_lags_lr}\n",
    "   - LocalStandardScaler for normalization\n",
    "   - Date features: day_of_week, day, week, is_weekend\n",
    "\n",
    "2. **For Experimentation**:\n",
    "   - Try more sophisticated models (ARIMA, Prophet)\n",
    "   - Add exogenous features (holidays, promotions, weather)\n",
    "   - Experiment with different loss functions\n",
    "\n",
    "3. **For Monitoring**:\n",
    "   - Track MASE and RMSSE over time\n",
    "   - Retrain when performance degrades\n",
    "   - Monitor for distribution shifts\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- üîÑ Implement automated retraining pipeline\n",
    "- üìä Add prediction intervals for uncertainty quantification\n",
    "- üöÄ Deploy model to production environment\n",
    "- üìà Set up monitoring dashboards\n",
    "\n",
    "---\n",
    "\n",
    "### üéì Congratulations!\n",
    "\n",
    "You've completed a full end-to-end time series forecasting workflow! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final summary with actual values\n",
    "print(\"=\"*80)\n",
    "print(\"                    WORKSHOP SUMMARY                    \")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìä Dataset: Top 50 Food Items\")\n",
    "print(f\"   - Items: {df['unique_id'].nunique()}\")\n",
    "print(f\"   - Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"   - Total observations: {len(df):,}\")\n",
    "\n",
    "print(f\"\\nüéØ Best Model: {best_model_mase}\")\n",
    "print(f\"   - MASE: {comparison_df.loc[best_model_mase, 'mase']:.4f}\")\n",
    "print(f\"   - RMSSE: {comparison_df.loc[best_model_mase, 'rmsse']:.4f}\")\n",
    "print(f\"   - Performance: {'‚úÖ Beats Naive' if comparison_df.loc[best_model_mase, 'mase'] < 1.0 else '‚ùå Below Naive'}\")\n",
    "\n",
    "print(f\"\\nüìà Average Seasonality Strength: {strength_df['Seasonality_Strength'].mean():.3f}\")\n",
    "print(f\"   - Strong weekly patterns detected\")\n",
    "print(f\"   - Peak sales day: {peak_day}\")\n",
    "\n",
    "print(f\"\\nüîß Optimal Configuration:\")\n",
    "if best_model_mase == 'LightGBM':\n",
    "    print(f\"   - Model: LightGBM\")\n",
    "    print(f\"   - Lags: {best_lags_lgbm}\")\n",
    "    print(f\"   - Feature Engineering: LocalStandardScaler + Date Features\")\n",
    "elif best_model_mase == 'LinearRegression':\n",
    "    print(f\"   - Model: Linear Regression\")\n",
    "    print(f\"   - Lags: {best_lags_lr}\")\n",
    "    print(f\"   - Feature Engineering: LocalStandardScaler + Date Features\")\n",
    "else:\n",
    "    print(f\"   - Model: {best_model_mase}\")\n",
    "    print(f\"   - Automatic component selection\")\n",
    "\n",
    "print(f\"\\nüí° Key Takeaways:\")\n",
    "print(f\"   1. Feature engineering improves performance by ~{improvement:.1f}%\")\n",
    "print(f\"   2. Cross-validation essential for hyperparameter tuning\")\n",
    "print(f\"   3. Scaled metrics (MASE, RMSSE) provide interpretable evaluation\")\n",
    "print(f\"   4. Global models more efficient than local approaches\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"               ‚úÖ WORKSHOP COMPLETE! üéâ\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
