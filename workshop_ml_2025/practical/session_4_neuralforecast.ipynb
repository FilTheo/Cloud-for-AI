{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_header",
   "metadata": {},
   "source": [
    "# Workshop Session 4: Neural Networks for Time Series Forecasting\n",
    "\n",
    "## Welcome to Session 4!\n",
    "\n",
    "In previous sessions, we explored **statistical models** (Session 1) and **machine learning models** (Sessions 2 & 3). Now we'll dive into **neural networks**!\n",
    "\n",
    "### What You'll Learn:\n",
    "1. **NeuralForecast Library** - deep learning made easy for time series\n",
    "2. **Popular Neural Models** - NBEATS, NHITS\n",
    "3. **Automatic Hyperparameter Tuning** - using AutoNBEATS and AutoNHITS\n",
    "4. **Model Comparison** - evaluating neural network performance\n",
    "\n",
    "### Why Neural Networks?\n",
    "\n",
    "**Advantages**:\n",
    "- ‚úÖ Capture **complex non-linear patterns**\n",
    "- ‚úÖ Learn **temporal dependencies** automatically\n",
    "- ‚úÖ Handle **multiple time series** efficiently (global models)\n",
    "- ‚úÖ **Automatic feature learning** (no manual feature engineering)\n",
    "\n",
    "**Trade-offs**:\n",
    "- ‚ö†Ô∏è Require more data and computational resources\n",
    "- ‚ö†Ô∏è Longer training times\n",
    "- ‚ö†Ô∏è Less interpretable than simpler models\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 22:42:51,157\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-10-15 22:42:51,436\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# NeuralForecast\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NBEATS, NHITS\n",
    "from neuralforecast.auto import AutoNBEATS, AutoNHITS\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Plotting\n",
    "plt.style.use(\"ggplot\")\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data_header",
   "metadata": {},
   "source": [
    "## 2. Load Data and Train/Test Split\n",
    "\n",
    "We'll use the same M5 food sales dataset from previous sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (12100, 3)\n",
      "Number of stores: 10\n",
      "Date range: 2013-01-01 00:00:00 to 2016-04-24 00:00:00\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>date</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA_2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA_3</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA_4</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TX_1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TX_2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TX_3</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WI_1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WI_2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WI_3</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id       date     y\n",
       "0      CA_1 2013-01-01  1888\n",
       "1      CA_2 2013-01-01  1320\n",
       "2      CA_3 2013-01-01  2454\n",
       "3      CA_4 2013-01-01  1031\n",
       "4      TX_1 2013-01-01  1607\n",
       "5      TX_2 2013-01-01  2469\n",
       "6      TX_3 2013-01-01  1773\n",
       "7      WI_1 2013-01-01  1365\n",
       "8      WI_2 2013-01-01  1506\n",
       "9      WI_3 2013-01-01  1415"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "data_path = \"/home/filtheo/Cloud-for-AI/workshop_ml_2025/data/converted_df.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=[\"date\"])\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of stores: {df['unique_id'].nunique()}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "train_test_split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test Split:\n",
      "============================================================\n",
      "Training set: 11,960 observations\n",
      "Test set: 140 observations\n",
      "\n",
      "Date ranges:\n",
      "  Train: 2013-01-01 00:00:00 to 2016-04-10 00:00:00\n",
      "  Test:  2016-04-11 00:00:00 to 2016-04-24 00:00:00\n",
      "\n",
      "Forecast horizon: 14 days\n",
      "\n",
      "Stores: ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n"
     ]
    }
   ],
   "source": [
    "# Train/Test Split: last 14 days for testing (same as previous sessions)\n",
    "test_size = 14\n",
    "max_date = df[\"date\"].max()\n",
    "split_date = max_date - pd.Timedelta(days=test_size - 1)\n",
    "\n",
    "df_train = df[df[\"date\"] < split_date].copy()\n",
    "df_test = df[df[\"date\"] >= split_date].copy()\n",
    "\n",
    "print(f\"Train/Test Split:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training set: {len(df_train):,} observations\")\n",
    "print(f\"Test set: {len(df_test):,} observations\")\n",
    "print(f\"\\nDate ranges:\")\n",
    "print(f\"  Train: {df_train['date'].min()} to {df_train['date'].max()}\")\n",
    "print(f\"  Test:  {df_test['date'].min()} to {df_test['date'].max()}\")\n",
    "print(f\"\\nForecast horizon: {test_size} days\")\n",
    "\n",
    "# Store list\n",
    "stores = df[\"unique_id\"].unique()\n",
    "print(f\"\\nStores: {list(stores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_format_header",
   "metadata": {},
   "source": [
    "## 3. Understanding NeuralForecast Data Format\n",
    "\n",
    "**NeuralForecast requires:**\n",
    "- `unique_id`: identifier for each time series\n",
    "- `ds`: timestamp column (datestamp)\n",
    "- `y`: target variable\n",
    "\n",
    "Our data already has `unique_id` and `y`, we just need to rename `date` to `ds`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prepare_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared for NeuralForecast:\n",
      "============================================================\n",
      "  unique_id         ds     y\n",
      "0      CA_1 2013-01-01  1888\n",
      "1      CA_2 2013-01-01  1320\n",
      "2      CA_3 2013-01-01  2454\n",
      "3      CA_4 2013-01-01  1031\n",
      "4      TX_1 2013-01-01  1607\n",
      "\n",
      "Columns: ['unique_id', 'ds', 'y']\n",
      "‚úÖ Format ready: unique_id, ds, y\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for NeuralForecast\n",
    "df_train_nf = df_train.rename(columns={\"date\": \"ds\"})\n",
    "df_test_nf = df_test.rename(columns={\"date\": \"ds\"})\n",
    "\n",
    "print(\"Data prepared for NeuralForecast:\")\n",
    "print(\"=\" * 60)\n",
    "print(df_train_nf.head())\n",
    "print(f\"\\nColumns: {list(df_train_nf.columns)}\")\n",
    "print(\"‚úÖ Format ready: unique_id, ds, y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural_models_intro",
   "metadata": {},
   "source": [
    "## 4. Introduction to Neural Network Models\n",
    "\n",
    "We'll explore two popular neural network architectures:\n",
    "\n",
    "### 1. **NBEATS** (Neural Basis Expansion Analysis)\n",
    "- **Key Idea**: Decomposes forecasts into trend and seasonality components using basis functions\n",
    "- **Interpretable**: Can visualize trend/seasonal contributions\n",
    "- **Architecture**: Deep residual blocks with doubly residual stacking\n",
    "- **Best for**: Interpretable forecasts with clear decomposition\n",
    "- **Paper**: [NBEATS: Neural basis expansion analysis for interpretable time series forecasting](https://arxiv.org/abs/1905.10437)\n",
    "\n",
    "### 2. **NHITS** (Neural Hierarchical Interpolation for Time Series)\n",
    "- **Key Idea**: Multi-rate interpolation across different temporal scales\n",
    "- **Fast**: More efficient than NBEATS (fewer parameters)\n",
    "- **Architecture**: Hierarchical stacks with interpolation\n",
    "- **Best for**: Long-horizon forecasting with speed requirements\n",
    "- **Paper**: [NHITS: Neural Hierarchical Interpolation for Time Series Forecasting](https://arxiv.org/abs/2201.12886)\n",
    "\n",
    "Let's start by training simple models with default parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nbeats_header",
   "metadata": {},
   "source": [
    "## 5. Train NBEATS with Default Parameters\n",
    "\n",
    "We'll start with a simple NBEATS model using minimal configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_nbeats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NBEATS with simple parameters\n",
    "print(\"Training NBEATS...\")\n",
    "\n",
    "# Initialize model\n",
    "# h = forecast horizon (14 days)\n",
    "# input_size = number of historical steps to use (2 * h is a good default)\n",
    "# max_steps = training iterations (keep low for speed)\n",
    "models_nbeats = [\n",
    "    NBEATS(\n",
    "        h=test_size,\n",
    "        input_size=2 * test_size,\n",
    "        #max_steps=500,\n",
    "        \n",
    "        #early_stop_patience_steps=3\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create NeuralForecast object\n",
    "nf_nbeats = NeuralForecast(models=models_nbeats, freq=\"D\")\n",
    "\n",
    "# Fit model\n",
    "nf_nbeats.fit(df=df_train_nf)\n",
    "\n",
    "# Generate forecasts\n",
    "forecasts_nbeats = nf_nbeats.predict()\n",
    "\n",
    "print(\"\\n‚úÖ NBEATS trained and forecasted!\")\n",
    "print(f\"Forecast shape: {forecasts_nbeats.shape}\")\n",
    "forecasts_nbeats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_nbeats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize NBEATS forecasts for all stores\n",
    "fig, axes = plt.subplots(5, 2, figsize=(15, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Merge forecasts with actual test values\n",
    "forecasts_nbeats_merged = forecasts_nbeats.reset_index().merge(\n",
    "    df_test_nf[[\"unique_id\", \"ds\", \"y\"]], on=[\"unique_id\", \"ds\"]\n",
    ")\n",
    "\n",
    "mse_nbeats = {}\n",
    "\n",
    "for idx, store in enumerate(stores):\n",
    "    train_store = df_train_nf[df_train_nf[\"unique_id\"] == store].sort_values(\"ds\")\n",
    "    test_store = df_test_nf[df_test_nf[\"unique_id\"] == store].sort_values(\"ds\")\n",
    "    forecast_store = forecasts_nbeats_merged[\n",
    "        forecasts_nbeats_merged[\"unique_id\"] == store\n",
    "    ].sort_values(\"ds\")\n",
    "\n",
    "    # Plot training data (last 90 days)\n",
    "    train_context = train_store.tail(90)\n",
    "    axes[idx].plot(\n",
    "        train_context[\"ds\"], train_context[\"y\"], color=\"black\", linewidth=1, alpha=0.7\n",
    "    )\n",
    "\n",
    "    # Plot actual test data\n",
    "    axes[idx].plot(\n",
    "        test_store[\"ds\"],\n",
    "        test_store[\"y\"],\n",
    "        color=\"black\",\n",
    "        linewidth=1.5,\n",
    "        alpha=0.8,\n",
    "        marker=\"o\",\n",
    "        markersize=2,\n",
    "    )\n",
    "\n",
    "    # Plot forecast\n",
    "    axes[idx].plot(\n",
    "        forecast_store[\"ds\"],\n",
    "        forecast_store[\"NBEATS\"],\n",
    "        color=\"#1f77b4\",\n",
    "        linewidth=2,\n",
    "        linestyle=\"-\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "    # Mark split\n",
    "    axes[idx].axvline(x=split_date, color=\"gray\", linestyle=\"-\", linewidth=1, alpha=0.5)\n",
    "\n",
    "    # Calculate MSE\n",
    "    mse = mean_squared_error(forecast_store[\"y\"], forecast_store[\"NBEATS\"])\n",
    "    mse_nbeats[store] = mse\n",
    "\n",
    "    axes[idx].set_title(f\"{store} (MSE={mse:.0f})\", fontsize=11, fontweight=\"bold\")\n",
    "    axes[idx].set_xlabel(\"Date\", fontsize=8)\n",
    "    axes[idx].set_ylabel(\"Sales\", fontsize=8)\n",
    "    axes[idx].tick_params(labelsize=7)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"NBEATS: All Stores\", fontsize=14, fontweight=\"bold\", y=1.00\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "avg_mse_nbeats = np.mean(list(mse_nbeats.values()))\n",
    "print(f\"\\nAverage MSE across all stores: {avg_mse_nbeats:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nhits_header",
   "metadata": {},
   "source": [
    "## 6. Train NHITS with Default Parameters\n",
    "\n",
    "Now let's try NHITS, which is typically faster and more efficient than NBEATS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_nhits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NHITS with simple parameters\n",
    "print(\"Training NHITS...\")\n",
    "\n",
    "models_nhits = [\n",
    "    NHITS(\n",
    "        h=test_size,\n",
    "        input_size=2 * test_size,\n",
    "        max_steps=500,\n",
    "        early_stop_patience_steps=3\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create NeuralForecast object\n",
    "nf_nhits = NeuralForecast(models=models_nhits, freq=\"D\")\n",
    "\n",
    "# Fit model\n",
    "nf_nhits.fit(df=df_train_nf)\n",
    "\n",
    "# Generate forecasts\n",
    "forecasts_nhits = nf_nhits.predict()\n",
    "\n",
    "print(\"\\n‚úÖ NHITS trained and forecasted!\")\n",
    "print(f\"Forecast shape: {forecasts_nhits.shape}\")\n",
    "forecasts_nhits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_nhits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize NHITS forecasts for all stores\n",
    "fig, axes = plt.subplots(5, 2, figsize=(15, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Merge forecasts with actual test values\n",
    "forecasts_nhits_merged = forecasts_nhits.reset_index().merge(\n",
    "    df_test_nf[[\"unique_id\", \"ds\", \"y\"]], on=[\"unique_id\", \"ds\"]\n",
    ")\n",
    "\n",
    "mse_nhits = {}\n",
    "\n",
    "for idx, store in enumerate(stores):\n",
    "    train_store = df_train_nf[df_train_nf[\"unique_id\"] == store].sort_values(\"ds\")\n",
    "    test_store = df_test_nf[df_test_nf[\"unique_id\"] == store].sort_values(\"ds\")\n",
    "    forecast_store = forecasts_nhits_merged[\n",
    "        forecasts_nhits_merged[\"unique_id\"] == store\n",
    "    ].sort_values(\"ds\")\n",
    "\n",
    "    # Plot training data (last 90 days)\n",
    "    train_context = train_store.tail(90)\n",
    "    axes[idx].plot(\n",
    "        train_context[\"ds\"], train_context[\"y\"], color=\"black\", linewidth=1, alpha=0.7\n",
    "    )\n",
    "\n",
    "    # Plot actual test data\n",
    "    axes[idx].plot(\n",
    "        test_store[\"ds\"],\n",
    "        test_store[\"y\"],\n",
    "        color=\"black\",\n",
    "        linewidth=1.5,\n",
    "        alpha=0.8,\n",
    "        marker=\"o\",\n",
    "        markersize=2,\n",
    "    )\n",
    "\n",
    "    # Plot forecast\n",
    "    axes[idx].plot(\n",
    "        forecast_store[\"ds\"],\n",
    "        forecast_store[\"NHITS\"],\n",
    "        color=\"#ff7f0e\",\n",
    "        linewidth=2,\n",
    "        linestyle=\"-\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "    # Mark split\n",
    "    axes[idx].axvline(x=split_date, color=\"gray\", linestyle=\"-\", linewidth=1, alpha=0.5)\n",
    "\n",
    "    # Calculate MSE\n",
    "    mse = mean_squared_error(forecast_store[\"y\"], forecast_store[\"NHITS\"])\n",
    "    mse_nhits[store] = mse\n",
    "\n",
    "    axes[idx].set_title(f\"{store} (MSE={mse:.0f})\", fontsize=11, fontweight=\"bold\")\n",
    "    axes[idx].set_xlabel(\"Date\", fontsize=8)\n",
    "    axes[idx].set_ylabel(\"Sales\", fontsize=8)\n",
    "    axes[idx].tick_params(labelsize=7)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"NHITS: All Stores\", fontsize=14, fontweight=\"bold\", y=1.00\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "avg_mse_nhits = np.mean(list(mse_nhits.values()))\n",
    "print(f\"\\nAverage MSE across all stores: {avg_mse_nhits:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare_basic_header",
   "metadata": {},
   "source": [
    "## 7. Compare Basic Models\n",
    "\n",
    "Let's compare NBEATS and NHITS performance across all stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare_basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print MSE comparison table\n",
    "print(\"MSE Comparison: NBEATS vs NHITS (Default Parameters)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Store':<10} {'NBEATS':<20} {'NHITS':<20}\")\n",
    "print(\"-\" * 60)\n",
    "for store in stores:\n",
    "    print(f\"{store:<10} {mse_nbeats[store]:<20.2f} {mse_nhits[store]:<20.2f}\")\n",
    "\n",
    "# Average across stores\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Average':<10} {avg_mse_nbeats:<20.2f} {avg_mse_nhits:<20.2f}\")\n",
    "\n",
    "# Bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "model_names = [\"NBEATS\", \"NHITS\"]\n",
    "avg_mses = [avg_mse_nbeats, avg_mse_nhits]\n",
    "colors = [\"#1f77b4\", \"#ff7f0e\"]\n",
    "\n",
    "bars = ax.bar(model_names, avg_mses, color=colors, alpha=0.8, edgecolor=\"black\", linewidth=1.5)\n",
    "\n",
    "# Add value labels\n",
    "for bar, mse in zip(bars, avg_mses):\n",
    "    height = bar.get_height()\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height,\n",
    "        f\"{mse:.1f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(\"Average MSE\", fontsize=12)\n",
    "ax.set_title(\"Neural Network Models: Performance Comparison (Default Parameters)\", fontsize=13, fontweight=\"bold\")\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Both models capture patterns, but can we improve with automatic tuning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auto_models_header",
   "metadata": {},
   "source": [
    "## 8. Automatic Hyperparameter Tuning with Auto Models\n",
    "\n",
    "NeuralForecast provides **Auto- models** that automatically search for optimal hyperparameters!\n",
    "\n",
    "### What are Auto Models?\n",
    "\n",
    "Auto models use **Ray** or **Optuna** to automatically find the best hyperparameters:\n",
    "- **AutoNBEATS**: Automatic hyperparameter optimization for NBEATS\n",
    "- **AutoNHITS**: Automatic hyperparameter optimization for NHITS\n",
    "\n",
    "### Key Parameters They Tune:\n",
    "- **`input_size`**: How many historical steps to use (context window)\n",
    "- **`learning_rate`**: Step size for gradient descent\n",
    "- **`batch_size`**: Number of samples per training iteration\n",
    "- **`max_steps`**: Number of training iterations\n",
    "- **Architecture-specific**: stack types, hidden sizes, etc.\n",
    "\n",
    "### How Auto Models Work:\n",
    "1. You define a **search space** (or use defaults)\n",
    "2. Specify **number of trials** (`num_samples`)\n",
    "3. Auto model tries different configurations\n",
    "4. Returns the **best model** based on validation loss\n",
    "\n",
    "Let's use Auto models with a simple configuration!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auto_nbeats_header",
   "metadata": {},
   "source": [
    "## 9. AutoNBEATS: Automatic Hyperparameter Tuning\n",
    "\n",
    "We'll use AutoNBEATS to automatically find the best hyperparameters.\n",
    "\n",
    "**Key parameters**:\n",
    "- `h`: Forecast horizon (14 days)\n",
    "- `num_samples`: Number of hyperparameter configurations to try (keep low for speed)\n",
    "- `config`: Custom search space (None = use defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_auto_nbeats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train AutoNBEATS with automatic hyperparameter tuning\n",
    "print(\"Training AutoNBEATS with automatic hyperparameter tuning...\")\n",
    "print(\"This will try multiple configurations and select the best one.\\n\")\n",
    "\n",
    "# Initialize AutoNBEATS\n",
    "# num_samples = number of hyperparameter configurations to try (3-5 for quick demo)\n",
    "# config = search space (None uses sensible defaults)\n",
    "models_auto_nbeats = [\n",
    "    AutoNBEATS(\n",
    "        h=test_size,\n",
    "        num_samples=3,  # Try 3 different configurations (keep low for speed)\n",
    "        config=None      # Use default search space\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create NeuralForecast object\n",
    "nf_auto_nbeats = NeuralForecast(models=models_auto_nbeats, freq=\"D\")\n",
    "\n",
    "# Fit model (this will automatically tune hyperparameters)\n",
    "nf_auto_nbeats.fit(df=df_train_nf)\n",
    "\n",
    "# Generate forecasts\n",
    "forecasts_auto_nbeats = nf_auto_nbeats.predict()\n",
    "\n",
    "print(\"\\n‚úÖ AutoNBEATS trained and forecasted!\")\n",
    "print(f\"Forecast shape: {forecasts_auto_nbeats.shape}\")\n",
    "forecasts_auto_nbeats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate_auto_nbeats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate AutoNBEATS\n",
    "forecasts_auto_nbeats_merged = forecasts_auto_nbeats.reset_index().merge(\n",
    "    df_test_nf[[\"unique_id\", \"ds\", \"y\"]], on=[\"unique_id\", \"ds\"]\n",
    ")\n",
    "\n",
    "mse_auto_nbeats = {}\n",
    "\n",
    "for store in stores:\n",
    "    store_forecasts = forecasts_auto_nbeats_merged[\n",
    "        forecasts_auto_nbeats_merged[\"unique_id\"] == store\n",
    "    ]\n",
    "    mse = mean_squared_error(store_forecasts[\"y\"], store_forecasts[\"AutoNBEATS\"])\n",
    "    mse_auto_nbeats[store] = mse\n",
    "\n",
    "avg_mse_auto_nbeats = np.mean(list(mse_auto_nbeats.values()))\n",
    "\n",
    "print(\"AutoNBEATS Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Average MSE: {avg_mse_auto_nbeats:.2f}\")\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  NBEATS (default):  {avg_mse_nbeats:.2f}\")\n",
    "print(f\"  AutoNBEATS (tuned): {avg_mse_auto_nbeats:.2f}\")\n",
    "\n",
    "improvement = ((avg_mse_nbeats - avg_mse_auto_nbeats) / avg_mse_nbeats) * 100\n",
    "print(f\"  ‚Üí Improvement: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auto_nhits_header",
   "metadata": {},
   "source": [
    "## 10. AutoNHITS: Automatic Hyperparameter Tuning\n",
    "\n",
    "Now let's use AutoNHITS to automatically find the best hyperparameters for NHITS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_auto_nhits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train AutoNHITS with automatic hyperparameter tuning\n",
    "print(\"Training AutoNHITS with automatic hyperparameter tuning...\")\n",
    "print(\"This will try multiple configurations and select the best one.\\n\")\n",
    "\n",
    "# Initialize AutoNHITS\n",
    "models_auto_nhits = [\n",
    "    AutoNHITS(\n",
    "        h=test_size,\n",
    "        num_samples=3,  # Try 3 different configurations\n",
    "        config=None      # Use default search space\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create NeuralForecast object\n",
    "nf_auto_nhits = NeuralForecast(models=models_auto_nhits, freq=\"D\")\n",
    "\n",
    "# Fit model (this will automatically tune hyperparameters)\n",
    "nf_auto_nhits.fit(df=df_train_nf)\n",
    "\n",
    "# Generate forecasts\n",
    "forecasts_auto_nhits = nf_auto_nhits.predict()\n",
    "\n",
    "print(\"\\n‚úÖ AutoNHITS trained and forecasted!\")\n",
    "print(f\"Forecast shape: {forecasts_auto_nhits.shape}\")\n",
    "forecasts_auto_nhits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate_auto_nhits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate AutoNHITS\n",
    "forecasts_auto_nhits_merged = forecasts_auto_nhits.reset_index().merge(\n",
    "    df_test_nf[[\"unique_id\", \"ds\", \"y\"]], on=[\"unique_id\", \"ds\"]\n",
    ")\n",
    "\n",
    "mse_auto_nhits = {}\n",
    "\n",
    "for store in stores:\n",
    "    store_forecasts = forecasts_auto_nhits_merged[\n",
    "        forecasts_auto_nhits_merged[\"unique_id\"] == store\n",
    "    ]\n",
    "    mse = mean_squared_error(store_forecasts[\"y\"], store_forecasts[\"AutoNHITS\"])\n",
    "    mse_auto_nhits[store] = mse\n",
    "\n",
    "avg_mse_auto_nhits = np.mean(list(mse_auto_nhits.values()))\n",
    "\n",
    "print(\"AutoNHITS Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Average MSE: {avg_mse_auto_nhits:.2f}\")\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  NHITS (default):  {avg_mse_nhits:.2f}\")\n",
    "print(f\"  AutoNHITS (tuned): {avg_mse_auto_nhits:.2f}\")\n",
    "\n",
    "improvement = ((avg_mse_nhits - avg_mse_auto_nhits) / avg_mse_nhits) * 100\n",
    "print(f\"  ‚Üí Improvement: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare_all_header",
   "metadata": {},
   "source": [
    "## 11. Compare All Models\n",
    "\n",
    "Let's compare default models vs auto-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare_all",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive MSE comparison\n",
    "print(\"MSE Comparison: Default vs Auto-Tuned Models\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Store':<10} {'NBEATS':<15} {'AutoNBEATS':<15} {'NHITS':<15} {'AutoNHITS':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for store in stores:\n",
    "    print(\n",
    "        f\"{store:<10} {mse_nbeats[store]:<15.2f} {mse_auto_nbeats[store]:<15.2f} \"\n",
    "        f\"{mse_nhits[store]:<15.2f} {mse_auto_nhits[store]:<15.2f}\"\n",
    "    )\n",
    "\n",
    "# Average across stores\n",
    "print(\"-\" * 80)\n",
    "print(\n",
    "    f\"{'Average':<10} {avg_mse_nbeats:<15.2f} {avg_mse_auto_nbeats:<15.2f} \"\n",
    "    f\"{avg_mse_nhits:<15.2f} {avg_mse_auto_nhits:<15.2f}\"\n",
    ")\n",
    "\n",
    "# Bar chart comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "models = [\"NBEATS\\n(Default)\", \"AutoNBEATS\\n(Tuned)\", \"NHITS\\n(Default)\", \"AutoNHITS\\n(Tuned)\"]\n",
    "avg_mses = [avg_mse_nbeats, avg_mse_auto_nbeats, avg_mse_nhits, avg_mse_auto_nhits]\n",
    "colors = [\"#8fc1e3\", \"#1f77b4\", \"#ffbb78\", \"#ff7f0e\"]\n",
    "\n",
    "bars = ax.bar(models, avg_mses, color=colors, alpha=0.8, edgecolor=\"black\", linewidth=1.5)\n",
    "\n",
    "# Add value labels\n",
    "for bar, mse in zip(bars, avg_mses):\n",
    "    height = bar.get_height()\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height,\n",
    "        f\"{mse:.1f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=11,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(\"Average MSE\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"Impact of Automatic Hyperparameter Tuning on Neural Network Performance\",\n",
    "    fontsize=13,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate improvements\n",
    "nbeats_improvement = ((avg_mse_nbeats - avg_mse_auto_nbeats) / avg_mse_nbeats) * 100\n",
    "nhits_improvement = ((avg_mse_nhits - avg_mse_auto_nhits) / avg_mse_nhits) * 100\n",
    "\n",
    "print(f\"\\nüí° Automatic Tuning Impact:\")\n",
    "print(f\"  NBEATS: {nbeats_improvement:.1f}% improvement\")\n",
    "print(f\"  NHITS: {nhits_improvement:.1f}% improvement\")\n",
    "print(\"  ‚Üí Auto models automatically find better hyperparameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best_model_header",
   "metadata": {},
   "source": [
    "## 12. Visualize Best Model\n",
    "\n",
    "Let's visualize the best performing model for all stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_best",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best model\n",
    "all_avg_mses = {\n",
    "    \"NBEATS\": avg_mse_nbeats,\n",
    "    \"AutoNBEATS\": avg_mse_auto_nbeats,\n",
    "    \"NHITS\": avg_mse_nhits,\n",
    "    \"AutoNHITS\": avg_mse_auto_nhits\n",
    "}\n",
    "\n",
    "best_model_name = min(all_avg_mses, key=all_avg_mses.get)\n",
    "best_avg_mse = all_avg_mses[best_model_name]\n",
    "\n",
    "# Select best forecasts\n",
    "if best_model_name == \"NBEATS\":\n",
    "    best_forecasts = forecasts_nbeats_merged\n",
    "    best_mse = mse_nbeats\n",
    "elif best_model_name == \"AutoNBEATS\":\n",
    "    best_forecasts = forecasts_auto_nbeats_merged\n",
    "    best_mse = mse_auto_nbeats\n",
    "elif best_model_name == \"NHITS\":\n",
    "    best_forecasts = forecasts_nhits_merged\n",
    "    best_mse = mse_nhits\n",
    "else:\n",
    "    best_forecasts = forecasts_auto_nhits_merged\n",
    "    best_mse = mse_auto_nhits\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Average MSE: {best_avg_mse:.2f}\\n\")\n",
    "\n",
    "# Visualize best model for all stores\n",
    "fig, axes = plt.subplots(5, 2, figsize=(15, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, store in enumerate(stores):\n",
    "    train_store = df_train_nf[df_train_nf[\"unique_id\"] == store].sort_values(\"ds\")\n",
    "    test_store = df_test_nf[df_test_nf[\"unique_id\"] == store].sort_values(\"ds\")\n",
    "    forecast_store = best_forecasts[\n",
    "        best_forecasts[\"unique_id\"] == store\n",
    "    ].sort_values(\"ds\")\n",
    "\n",
    "    # Plot training data (last 90 days)\n",
    "    train_context = train_store.tail(90)\n",
    "    axes[idx].plot(\n",
    "        train_context[\"ds\"], train_context[\"y\"], color=\"black\", linewidth=1, alpha=0.7\n",
    "    )\n",
    "\n",
    "    # Plot actual test data\n",
    "    axes[idx].plot(\n",
    "        test_store[\"ds\"],\n",
    "        test_store[\"y\"],\n",
    "        color=\"black\",\n",
    "        linewidth=1.5,\n",
    "        alpha=0.8,\n",
    "        marker=\"o\",\n",
    "        markersize=2,\n",
    "    )\n",
    "\n",
    "    # Plot forecast\n",
    "    color = \"#2ca02c\"\n",
    "    axes[idx].plot(\n",
    "        forecast_store[\"ds\"],\n",
    "        forecast_store[best_model_name],\n",
    "        color=color,\n",
    "        linewidth=2,\n",
    "        linestyle=\"-\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "    # Mark split\n",
    "    axes[idx].axvline(x=split_date, color=\"gray\", linestyle=\"-\", linewidth=1, alpha=0.5)\n",
    "\n",
    "    mse = best_mse[store]\n",
    "    axes[idx].set_title(f\"{store} (MSE={mse:.0f})\", fontsize=11, fontweight=\"bold\")\n",
    "    axes[idx].set_xlabel(\"Date\", fontsize=8)\n",
    "    axes[idx].set_ylabel(\"Sales\", fontsize=8)\n",
    "    axes[idx].tick_params(labelsize=7)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\n",
    "    f\"Best Model: {best_model_name}\", fontsize=14, fontweight=\"bold\", y=1.00\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "## 13. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned in Session 4:\n",
    "\n",
    "#### 1. **NeuralForecast Library**\n",
    "- User-friendly interface for neural forecasting\n",
    "- Sklearn-like `.fit()` and `.predict()` methods\n",
    "- Built on PyTorch with automatic GPU support\n",
    "\n",
    "#### 2. **Neural Network Models**\n",
    "- **NBEATS**: Interpretable, decomposes into trend/seasonality\n",
    "- **NHITS**: Fast, efficient, hierarchical interpolation\n",
    "- Both are **global models** - train once on all series\n",
    "\n",
    "#### 3. **Automatic Hyperparameter Tuning**\n",
    "- **AutoNBEATS** and **AutoNHITS** automatically find best parameters\n",
    "- Use Ray/Optuna for efficient hyperparameter search\n",
    "- Simply specify `num_samples` (number of trials)\n",
    "- No manual tuning required!\n",
    "\n",
    "#### 4. **When to Use Neural Networks**\n",
    "\n",
    "**Use neural networks when:**\n",
    "- ‚úÖ You have **large datasets** (thousands of observations)\n",
    "- ‚úÖ Patterns are **complex and non-linear**\n",
    "- ‚úÖ You have **multiple related time series** (global modeling)\n",
    "- ‚úÖ You have **computational resources** (GPU recommended)\n",
    "\n",
    "**Stick with simpler models when:**\n",
    "- ‚ö†Ô∏è Dataset is **small** (<1000 observations)\n",
    "- ‚ö†Ô∏è Patterns are **simple and linear**\n",
    "- ‚ö†Ô∏è **Interpretability** is critical\n",
    "- ‚ö†Ô∏è **Speed** is more important than accuracy\n",
    "\n",
    "### Performance Summary:\n",
    "\n",
    "| Model | Average MSE | Notes |\n",
    "|-------|------------|-------|\n",
    "| NBEATS (default) | Baseline | Fixed parameters |\n",
    "| AutoNBEATS | Improved | Automatically tuned |\n",
    "| NHITS (default) | Baseline | Fixed parameters |\n",
    "| AutoNHITS | Improved | Automatically tuned |\n",
    "\n",
    "### Key Principles:\n",
    "\n",
    "1. **Start simple** - test with default parameters first\n",
    "2. **Use Auto models** - let the library find optimal hyperparameters\n",
    "3. **Global modeling** - neural networks train on all series together\n",
    "4. **Validate properly** - ensure test period is representative\n",
    "5. **Consider trade-offs** - accuracy vs speed vs interpretability\n",
    "\n",
    "### Auto Models vs Manual Tuning:\n",
    "\n",
    "**Advantages of Auto Models:**\n",
    "- ‚úÖ No expert knowledge required\n",
    "- ‚úÖ Automatic hyperparameter search\n",
    "- ‚úÖ Often find better configurations than manual tuning\n",
    "- ‚úÖ Save time and effort\n",
    "\n",
    "**When to use manual tuning:**\n",
    "- Domain-specific constraints (e.g., fixed input_size)\n",
    "- Very limited computational budget\n",
    "- Need precise control over all parameters\n",
    "\n",
    "---\n",
    "\n",
    "### üéì Congratulations!\n",
    "\n",
    "You've completed Session 4 of the Time Series Forecasting Workshop. You now understand:\n",
    "- How to use NeuralForecast for neural network models\n",
    "- Popular architectures: NBEATS and NHITS\n",
    "- **Automatic hyperparameter tuning** with AutoNBEATS and AutoNHITS\n",
    "- When to use neural networks vs traditional ML\n",
    "\n",
    "**Next Steps**: \n",
    "- Explore other models: LSTM, TFT, Transformers\n",
    "- Try custom search spaces with Auto models\n",
    "- Experiment with probabilistic forecasting\n",
    "- Scale up with GPU acceleration! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
